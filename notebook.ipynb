{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netwerkanalyse Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dit notebook wordt de code die gemaakt is door breinbaas omgezet in 1 notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mogr\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import geopandas as gpd\n",
    "\n",
    "import os\n",
    "import ogr\n",
    "import shapely\n",
    "import logging\n",
    "\n",
    "import xml.etree.ElementTree as E\n",
    "import requests\n",
    "from shapely.ops import unary_union\n",
    "import numpy as np\n",
    "import owslib.fes as fes\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import wget\n",
    "from tempfile import TemporaryDirectory\n",
    "from zipfile import ZipFile\n",
    "from time import sleep\n",
    "import shapely.ops\n",
    "from shapely.geometry import MultiLineString\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import MultiPolygon\n",
    "\n",
    "import shapely.wkt\n",
    "from owslib.wfs import WebFeatureService\n",
    "\n",
    "from http.client import HTTPException\n",
    "import time\n",
    "import zipfile\n",
    "import tempfile\n",
    "from osgeo import gdal\n",
    "\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Tuple\n",
    "from shapely.geometry import LineString, Polygon\n",
    "\n",
    "from math import hypot\n",
    "\n",
    "from shapely.geometry import MultiPoint, LineString, MultiPolygon, Point\n",
    "\n",
    "import shapefile\n",
    "\n",
    "from tqdm import tqdm\n",
    "from objects import Pump, Plot, WaterDeel, SewerLine\n",
    "\n",
    "# als FORCE_RELOAD op True staat dan worden eerder gecreeerde analyse resultaten opnieuw gegenereerd\n",
    "# dit duurt (veel) langer maar kan nodig zijn als bepaalde parameters veranderd zijn, bv de\n",
    "# afstand tussen pomp en leiding of de grootte van de buffer rondom de leiding etc.\n",
    "FORCE_RELOAD = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUNICIPALITIES = [\n",
    "    # \"Leiden\",\n",
    "    # \"Gouda\",\n",
    "    # \"Lisse\",\n",
    "    # \"Teylingen\",\n",
    "    \"Hillegom\",\n",
    "    # \"Katwijk\",\n",
    "    # \"Wassenaar\",\n",
    "    # \"Leiderdorp\",\n",
    "    # \"Noordwijk\",\n",
    "    # \"Zoeterwoude\",\n",
    "    # \"Bodegraven-Reeuwijk\",\n",
    "    # \"Haarlemmermeer\",\n",
    "    # \"Alphen aan den Rijn\",\n",
    "]\n",
    "\n",
    "MAX_PUMP_TO_SEWERLINE_DISTANCE = 10\n",
    "MAX_SEWER_LINE_TO_SEWER_LINE_DISTANCE = 1\n",
    "MAX_SEWER_LINE_TO_PLOT_DISTANCE = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataportaal > io > gwsw_wfs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GWSW_NETWERK_WFS_URL = 'https://geodata.gwsw.nl/{}/netwerk'\n",
    "GWSW_NETWERK_WFS_URL = \"https://geodata.gwsw.nl/geoserver/{}-netwerk/wfs\"\n",
    "GWSW_BEHEER_WFS_URL = \"https://geodata.gwsw.nl/geoserver/{}-beheer/wfs\"\n",
    "NAME_CONVERSION = {\n",
    "    \"Bodegraven-Reeuwijk\": \"Bodegraven\",\n",
    "    \"Haarlemmermeer\": \"Haarlemmermeer\",\n",
    "    \"Alphen aan den Rijn\": \"AlphenAanDenRijn\",\n",
    "    \"Zoeterwoude\": \"Zoeterwoude\",\n",
    "    \"Gouda\": \"Gouda\",\n",
    "    \"Wassenaar\": \"Wassenaar\",\n",
    "    \"Leiderdorp\": \"Leiderdorp\",\n",
    "    \"Leiden\": \"Leiden\",\n",
    "    \"Noordwijk\": \"Noordwijk\",\n",
    "    \"Lisse\": \"Lisse\",\n",
    "    \"Teylingen\": \"Teylingen\",\n",
    "    \"Hillegom\": \"Hillegom\",\n",
    "    \"Katwijk\": \"Katwijk\",\n",
    "    \"Voorschoten\": \"Voorschoten\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_gwsw_sewer_lines(city, data_folder, cache_data=False, filter_value=None):\n",
    "    \"\"\"\n",
    "    Gets data from gwsw WFS and filters only lines that are relevant for\n",
    "    'ongerioleerde percelen check.' Filters on these two types:\n",
    "    - 'http://data.gwsw.nl/1.5/totaal/GemengdRiool'\n",
    "    - 'http://data.gwsw.nl/1.5/totaal/Vuilwaterriool'\n",
    "    (Type 'http://data.gwsw.nl/1.5/totaal/Drukleiding' is removed because sinks were added.)\n",
    "    Uses url 'https://geodata.gwsw.nl/<city>/beheer' and feature name\n",
    "    'gwsw:Beheer_Leiding'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    city: str\n",
    "        The city name for which the data should be downloaded.\n",
    "    data_folder: str\n",
    "        The output folder where the output should be written to.\n",
    "    cache_data: bool\n",
    "        If True, the data will be saved as parquet file (2a_sewer_lines.parquet)\n",
    "    filter_value: array of str, optional\n",
    "        An array with the specific typenames to download.\n",
    "        Default value is ['http://data.gwsw.nl/1.5/totaal/GemengdRiool',\n",
    "                          'http://data.gwsw.nl/1.5/totaal/Vuilwaterriool']\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: geodataframe\n",
    "        Contains the requested data in crs EPSG:28992.\n",
    "    \"\"\"\n",
    "    if filter_value is None:\n",
    "        filter_value = (\n",
    "            r\"http://data.gwsw.nl/\\d.\\d/totaal/GemengdRiool\"\n",
    "            + r\"|http://data.gwsw.nl/\\d.\\d/totaal/Vuilwaterriool\"\n",
    "        )\n",
    "        # ['/totaal/GemengdRiool'|\n",
    "        # '/totaal/Vuilwaterriool'] # ,\n",
    "        # 'http://data.gwsw.nl/1.5/totaal/Drukleiding']\n",
    "    layer = \"gwsw:beheer_leiding\"\n",
    "    gwsw_city = NAME_CONVERSION[city]\n",
    "    wfs_url = GWSW_BEHEER_WFS_URL.format(gwsw_city)\n",
    "    data = get_wfs_data(wfs_url, \"2.0.0\", layer)\n",
    "    # data = data[filter_value.isin(data['type'])]\n",
    "    data = data.loc[data[\"type\"].str.match(filter_value)]\n",
    "    if cache_data:\n",
    "        path = os.path.join(data_folder, \"2a_sewer_lines.parquet\")\n",
    "        data.to_parquet(path)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_gwsw_pump_points(city, data_folder, cache_data=False, filter_value=None):\n",
    "    \"\"\"\n",
    "    Gets data from gwsw WFS and filters only lines that are relevant for\n",
    "    'ongerioleerde percelen check.' Filters daefault on these two types:\n",
    "    ['http://data.gwsw.nl/1.5/totaal/Rioolgemaal',\n",
    "     'http://data.gwsw.nl/1.5/totaal/Pompunit']\n",
    "\n",
    "    Uses url 'https://geodata.gwsw.nl/<city>/beheer' and feature name\n",
    "    'gwsw:Beheer_Pomp'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    city: str\n",
    "        The city name for which the data should be downloaded.\n",
    "    data_folder: str\n",
    "        The output folder where the output should be written to.\n",
    "    cache_data: bool\n",
    "        If True, the data will be saved as parquet file (2b_punp_points.parquet)\n",
    "    filter_value: array of str, optional\n",
    "        An array with the specific typenames to download.\n",
    "        Default value is ['http://data.gwsw.nl/1.5/totaal/Rioolgemaal',\n",
    "                          'http://data.gwsw.nl/1.5/totaal/Pompunit']\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: geodataframe\n",
    "        Contains the requested data in crs EPSG:28992.\n",
    "    \"\"\"\n",
    "    if filter_value is None:\n",
    "        filter_value = (\n",
    "            r\"http://data.gwsw.nl/\\d.\\d/totaal/Rioolgemaal\"\n",
    "            + r\"|http://data.gwsw.nl/\\d.\\d/totaal/Pompunit\"\n",
    "            + r\"|http://data.gwsw.nl/\\d.\\d/totaal/Gemaal\"\n",
    "            + r\"|http://data.gwsw.nl/\\d.\\d/totaal/Pompput\"\n",
    "        )\n",
    "    layer = \"gwsw:beheer_pomp\"\n",
    "    gwsw_city = NAME_CONVERSION[city]\n",
    "    wfs_url = GWSW_BEHEER_WFS_URL.format(gwsw_city)\n",
    "    data = get_wfs_data(wfs_url, \"2.0.0\", layer)\n",
    "    # data = data[data['type'].isin(filter_value)]\n",
    "    data = data.loc[data[\"type\"].str.match(filter_value)]\n",
    "\n",
    "    if cache_data:\n",
    "        path = os.path.join(data_folder, \"2b_pump_points.parquet\")\n",
    "        data.to_parquet(path)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_wfs_data(url, version, layer, srs=\"EPSG:28992\", outputformat=\"json\"):\n",
    "    \"\"\"\n",
    "    Gets featuredata from a WFS service and returns it a a geodataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        The base url of the WFS service.\n",
    "    version: str\n",
    "        The version of WFS to use.\n",
    "    layer: str\n",
    "        The name of the layer to get the data from.\n",
    "    srs: str, optional\n",
    "        The projection to use.\n",
    "        Default is EPSG:28992\n",
    "    outputformat: str, optional\n",
    "        Output format the service should provide.\n",
    "        Default is json.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: geodataframe\n",
    "        A geodataframe with the requested data.\n",
    "    \"\"\"\n",
    "    params = dict(\n",
    "        service=\"WFS\",\n",
    "        version=version,\n",
    "        request=\"GetFeature\",\n",
    "        typeName=layer,\n",
    "        srsname=srs,\n",
    "        outputFormat=outputformat,\n",
    "    )\n",
    "    # Parse the URL with parameters\n",
    "    req_url = Request(\"GET\", url, params=params).prepare().url\n",
    "\n",
    "    # Read data from URL, gives ERROR: Could not resolve host: geo,\n",
    "    # however it does return the correct data\n",
    "    data = gpd.read_file(req_url)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_wfs_connection(wfs_url, version):\n",
    "    \"\"\"\n",
    "    Connects to a WFS service and returns an WFS object and it's metadata.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    wfs_url: str\n",
    "        The url of the WFS service\n",
    "    version: str\n",
    "        The WFS version to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    wfs: WebFeatureService\n",
    "        An instance of owslib.wfs.WebFeatureService\n",
    "    contents: dict\n",
    "        A dictionary with the metadata of the WFS service.\n",
    "    \"\"\"\n",
    "    wfs = WebFeatureService(url=wfs_url, version=version)\n",
    "    # Get metadata like layernames etc.\n",
    "    return wfs, wfs.contents\n",
    "\n",
    "\n",
    "def save_as_type(path, gdf, output_format=\"GML\"):\n",
    "    \"\"\"\n",
    "    Saves a geodataframe to a given path in given format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        String with path for the output file\n",
    "    gdf: geodataframe\n",
    "        The geodataframe to export\n",
    "    output_format: str, optional\n",
    "        The type of file to export\n",
    "        Default is GML\n",
    "        Possible values are: 'AeronavFAA', 'ARCGEN',' BNA', 'DXF', 'CSV',\n",
    "        'OpenFileGDB','ESRIJSON', 'ESRI Shapefile', 'FlatGeobuf', 'GeoJSON',\n",
    "        'GeoJSONSeq', 'GPKG', 'GML', 'OGR_GMT', 'GPX', 'GPSTrackMaker',\n",
    "        'Idrisi', 'MapInfo File', 'DGN', 'PCIDSK', 'OGR_PDS', 'S57', 'SEGY',\n",
    "        'SUA', 'TopoJSON'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    gdf.to_file(path, driver=output_format)\n",
    "\n",
    "\n",
    "def read_geo_csv(filepath):\n",
    "    \"\"\"\n",
    "    Reads a geodataframe from a csv file. It is expected that the csv file has\n",
    "    a column geometry and crs EPSG:28992.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath: str\n",
    "        The path to the csv file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: geodataframe\n",
    "        The data as geodataframe with crs 28992.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filepath)\n",
    "    geometry = data[\"geometry\"].map(shapely.wkt.loads)\n",
    "    data = gpd.GeoDataFrame(data, crs=\"EPSG:28992\", geometry=geometry)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataportaal > io > pdok_api_kadestralekaart.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions to download kadaster plots.\n",
    "See explanation here:\n",
    "https://api.pdok.nl/kadaster/kadastralekaart/download/v4_0/ui/\n",
    "\n",
    "Every download is a full custom download, delta's are not used here.\n",
    "The downloaded parcels have a geometry of type:\n",
    "shapely.geometry.multilinestring.multilinestring\n",
    "\n",
    "These are converted to polygons, but this proces causes a lot of warnings.\n",
    "TODO: This part of the process needs a thorough review\n",
    "\"\"\"\n",
    "\n",
    "def get_gdf_plots(polygon, data_folder, cache_data=False):\n",
    "    \"\"\"\n",
    "    TODO: Discuss with Yanick to use the new functions ('create plots')\n",
    "    Returns a dataframe with kadaster plots that lie within a\n",
    "    given polygon.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    polygon: shapely polygon\n",
    "    data_folder: str\n",
    "    cache_data: bool, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gdf: geodataframe\n",
    "\n",
    "    \"\"\"\n",
    "    api_url = \"https://api.pdok.nl/kadaster/kadastralekaart/download/v5_0/full/custom\"\n",
    "\n",
    "    # TODO api is not functioning anymore..\n",
    "    # TODO replace by the new WFS server\n",
    "\n",
    "    gdfs = get_gdf_pdok_api(\n",
    "        str(polygon),\n",
    "        api_url,\n",
    "        layer_names=[\n",
    "            \"kadastralekaart_kadastralegrens.gml\",\n",
    "            \"kadastralekaart_perceel.gml\",\n",
    "        ],\n",
    "        format=\"gml\",\n",
    "        projection_crs=\"epsg:28992\",\n",
    "    )\n",
    "    gdf_grens = gdfs[0]  # Dataframe with all borders as Linestring or Multilinestring\n",
    "    gdf_perc = gdfs[1]  # Dataframe with administrative info about plots\n",
    "\n",
    "    # Limit the plots to the desired polygon of the municipality\n",
    "    polygon = polygon.buffer(\n",
    "        10\n",
    "    )  # make sure to include plots that are just on the border\n",
    "    gdf_perc = gdf_perc.loc[gdf_perc.within(polygon)]\n",
    "    # Drop the column geometry to prevent two geometry columns in the final result\n",
    "    gdf_perc = gdf_perc.drop(columns=[\"geometry\"])\n",
    "\n",
    "    logging.info(\"Parsing %s percelen, this might take some time\", len(gdf_perc.gml_id))\n",
    "    # gdf = kadastralegrens_to_perceel(gdf_perc, gdf_grens)\n",
    "    gdf = create_plots(gdf_perc, gdf_grens)\n",
    "\n",
    "    if cache_data:\n",
    "        path = os.path.join(data_folder, \"1_plots.parquet\")\n",
    "        gdf.to_parquet(path)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def create_polygon(collection):\n",
    "    \"\"\"\n",
    "    Creates a polygon from a collection of Multilinestrings and/or Linstrings\n",
    "    using the function shapely.ops.polygonize.\n",
    "    In case of polygons with holes: returns the right outer polygon.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    collection: geodataframe\n",
    "        The dataframe that contains the geometries with Linestrings and/or\n",
    "        Multilinestrings\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    final_result: polygon\n",
    "        The polygon representation of the input.\n",
    "\n",
    "    \"\"\"\n",
    "    final_result = None\n",
    "    polygons = list(shapely.ops.polygonize(collection.geometry))\n",
    "    num_polygons = len(polygons)\n",
    "    if num_polygons == 1:\n",
    "        final_result = polygons[0]\n",
    "    else:\n",
    "        n = 0\n",
    "        while (n < num_polygons - 1) | (final_result is None):\n",
    "            if len(polygons[n].interiors) == (num_polygons - 1):\n",
    "                final_result = polygons[n]\n",
    "            n = n + 1\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def create_plots(gdf_plots, gdf_borders):\n",
    "    \"\"\"\n",
    "    Receives the downloads from the kadastral website with plots and borders of the plots.\n",
    "    All borders are shapely.geometry.LineString or shapely.geometry.MultilineString.\n",
    "    All linestrings are merged and polygonized with shapely function polygonize.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf_plots: geodataframe\n",
    "        The geodataframe containing all information about the plots like id's etc.\n",
    "    gdf_borders: geodataframe\n",
    "        The geodataframe with the borders for all plots as downloaded from the kadastral website.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gdf_result: geodataframe\n",
    "        A dataframe with columns:\n",
    "        - gml_id: the Id of the plot\n",
    "        - geometry: the geometry of the plot as Polygon\n",
    "        - plus all columns of gdf_plots\n",
    "    \"\"\"\n",
    "    # # create geodataframe for the final result\n",
    "\n",
    "    gdf_result = gpd.GeoDataFrame(\n",
    "        columns=[\"gml_id\", \"geometry\"], geometry=\"geometry\", crs=\"epsg:28992\"\n",
    "    )\n",
    "\n",
    "    result = []\n",
    "\n",
    "    # loop through alle unique plot id's. Some id's only occur in one of these columns.\n",
    "    # note that since v5.0 the gml_id is defined as NL.IMKAD.KadastraalObject.23970659070000\n",
    "    # the perceelLinks and perceelRechts only contain the last 14 digits\n",
    "    for gml_id in gdf_plots[\"gml_id\"]:\n",
    "        # select all lineparts for the left and the right, merge these to one geometry\n",
    "        identificatie = int(gml_id.split(\".\")[-1])  # only get the 14 digits\n",
    "        collection = gdf_borders.loc[\n",
    "            (gdf_borders[\"perceelLinks\"] == identificatie)\n",
    "            | (gdf_borders[\"perceelRechts\"] == identificatie)\n",
    "        ][[\"geometry\"]]\n",
    "        try:\n",
    "            plot = create_polygon(collection)\n",
    "            row = gpd.GeoDataFrame(\n",
    "                [{\"gml_id\": gml_id, \"geometry\": plot}], crs=\"epsg:28992\"\n",
    "            )\n",
    "            gdf_result = gpd.GeoDataFrame(\n",
    "                pd.concat([gdf_result, row], ignore_index=True)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Many plots are partly outside the border, will result in error.\n",
    "            # The check is left out now probably this increases the speed.\n",
    "            logging.debug(f\"Perceel {gml_id} cannot be polygonized, got error '{e}'.\")\n",
    "\n",
    "    gdf_result = gdf_result.merge(gdf_plots, on=\"gml_id\", suffixes=[\"\", \"_perc\"])\n",
    "    return gdf_result.reset_index()\n",
    "\n",
    "\n",
    "def get_gdf_pdok_api(\n",
    "    polygon_str,\n",
    "    api_url,\n",
    "    layer_names=[\"kadastralekaart_kadastralegrens.gml\"],\n",
    "    format=\"gml\",\n",
    "    projection_crs=\"epsg:28992\",\n",
    "):\n",
    "    \"\"\"Download a gml file from PDOK API and read as GeoDataFrame.\n",
    "    Use polygon to filter the area you want to receive.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    polygon_str : str\n",
    "        polygon shape used as filter\n",
    "            kadastralekaart accepts format: POLYGON((x1 y1, x2 y2, x3 y3, x1 y1))\n",
    "            bgt accepts a list of bounding boxes: POLYGON(([xmin, ymin, xmax, ymax]))\n",
    "    api_url : str\n",
    "        PDOK API url.For kadastralekaart:\n",
    "        old; https://downloads.pdok.nl/kadastralekaart/api/v4_0/full/custom\n",
    "        new; https://api.pdok.nl/kadaster/kadastralekaart/download/v5_0/full/custom\n",
    "    layer_names: array of str\n",
    "        The names of the output files for the layers in the request. In the code\n",
    "        the featuretypes are 'perceel' and 'kadastralegrens'.\n",
    "    format: str\n",
    "        The format in which the data is downloaded. Default = 'gml'\n",
    "    projection_crs: str\n",
    "        The epsg code of the desired projection. Default = 'epsg:28992'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gdfs: array of geodataframe\n",
    "        Containing the requested PDOK data in two dataframes\n",
    "        (one containing plots, another containing the border of the frames)\n",
    "    \"\"\"\n",
    "    logging.info(f\"Send download request to PDOK kadastralekaart API server\")\n",
    "    # build request to API server\n",
    "    r = requests.post(\n",
    "        api_url,\n",
    "        json={\n",
    "            \"featuretypes\": [\"perceel\", \"kadastralegrens\"],\n",
    "            \"format\": format,\n",
    "            \"geofilter\": polygon_str,\n",
    "        },\n",
    "    )\n",
    "    logging.info(f\"response status code {r.status_code}\")\n",
    "\n",
    "    # check response\n",
    "    result = r.json()\n",
    "    downloadID = result[\"downloadRequestId\"]\n",
    "\n",
    "    sleep(5)\n",
    "\n",
    "    # Get status response\n",
    "    url_download = api_url + f\"/{downloadID}/status\"\n",
    "    response = requests.get(url_download)\n",
    "    status = response.json()\n",
    "\n",
    "    # Check progress status\n",
    "    while status[\"status\"] != \"COMPLETED\":\n",
    "        response = requests.get(url_download)\n",
    "        status = response.json()\n",
    "        ## This try except is build in by peter venema because suddenly errors\n",
    "        ## occured during download Haarlemmermeer (> 60.000 plots).\n",
    "        ## The error message was 'Too many requests'\n",
    "        try:\n",
    "            if divmod(status[\"progress\"], 1)[1] == 0:\n",
    "                job_progress(\"status of job: {} %\".format(status[\"progress\"]))\n",
    "            logging.info(f\"status response is {status['status']}\")\n",
    "        except Exception as e:\n",
    "            ## This is build in as workaround if server says too many requests.\n",
    "            ## Danger is that we will be blacklisted.\n",
    "            logging.error(\"Exception occured while downloading plots, %s\", status)\n",
    "            status[\"status\"] = \"RUNNING\"\n",
    "\n",
    "    # Perform zipfile download\n",
    "    with TemporaryDirectory() as temp_dir:\n",
    "        if status[\"status\"] == \"COMPLETED\":\n",
    "            logging.info(f\"Download zipfile\")\n",
    "            pdok_bgt_theme_zip = wget.download(\n",
    "                f\"https://api.pdok.nl{status['_links']['download']['href']}\",\n",
    "                os.path.join(temp_dir, \"pdok_download.zip\"),\n",
    "                bar=download_progress,\n",
    "            )\n",
    "\n",
    "        # Unpack downloaded zip file containing wanted gml's\n",
    "        with ZipFile(pdok_bgt_theme_zip, \"r\") as zipObj:\n",
    "            logging.info(f\"Extract zipfile\")\n",
    "            zipObj.extractall()\n",
    "\n",
    "        gdfs = []\n",
    "        logging.info(f\"Merge layers\")\n",
    "        for layer in layer_names:\n",
    "            gdf = gpd.read_file(layer)\n",
    "            gdf.crs = projection_crs\n",
    "            gdfs.append(gdf)\n",
    "    return gdfs\n",
    "\n",
    "\n",
    "def job_progress(progress_message):\n",
    "    \"\"\"Flushes progress message to stdout, i.e. a dynamic\n",
    "    changing message appears as notebook cell output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    progress_message : str\n",
    "        Progress message text\n",
    "    \"\"\"\n",
    "    sys.stdout.write(\"\\r\" + progress_message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def download_progress(current, total, width=80):\n",
    "    \"\"\"Flush download progress message to stdout, i.e.\n",
    "    a dynamic changing one line message appears as\n",
    "    notebook cell output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current : float\n",
    "        downloaded part of file so far\n",
    "    total : float\n",
    "        total size of file to download\n",
    "    width : int, optional\n",
    "        maximum allowed line width, by default 80\n",
    "    \"\"\"\n",
    "    progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (\n",
    "        current / total * 100,\n",
    "        current,\n",
    "        total,\n",
    "    )\n",
    "    sys.stdout.write(\"\\r\" + progress_message)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# OLD FUNCTIONS, TROW AWAY IN NEXT UPDATE PLEASE.\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "def old_kadastralegrens_to_perceel(gdf_perc, gdf_grens):\n",
    "    \"\"\"\n",
    "    NOT USED ANYMORE\n",
    "    TODO: @Yanick Mampaey: please explain better what happens here.\n",
    "\n",
    "    Creates polygons from two geodataframes from BRK.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf_perc: geodataframe\n",
    "        The dataframe that contains the id of a plot. Geometry is\n",
    "        a point.\n",
    "    gdf_grens: geodataframe\n",
    "        The dataframe that contains the actual coordinates of the\n",
    "        border of the plot. Geometry is of type\n",
    "        shapely.geometry.multilinestring.multilinestring\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gdf: geodataframe\n",
    "        A geodataframe with all plots as polygons.\n",
    "\n",
    "    \"\"\"\n",
    "    gdf = gdf_perc.copy(deep=True)\n",
    "\n",
    "    # gdf = gdf.explode() # to get rid of multipolygons --> no effect :-(\n",
    "    def get_cadasteral_boundaries(row):\n",
    "        \"\"\"\n",
    "        Internal function, is called as lambda function  in\n",
    "        apply on dataframe gdf_perc, a row from that\n",
    "        dataframe is passed.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        row: dataframe row\n",
    "            The row that contains a shapely geometry.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gdf: geodataframe\n",
    "        \"\"\"\n",
    "        geometry = gdf_grens[\n",
    "            (gdf_grens[\"perceelLinks|gml_id\"].values == row.gml_id)\n",
    "            | (gdf_grens[\"perceelRechts|gml_id\"].values == row.gml_id)\n",
    "        ].geometry.unary_union\n",
    "        # geometry is either a linestring or a multilinestring and need to be handeled seperately\n",
    "\n",
    "        try:  # for a simple linestring.\n",
    "            # this will always go wrong so seems to make no sense.\n",
    "            #     poly_coordinates = list(geometry.coords)\n",
    "            # except Exception as e:\n",
    "            # for the multilinestring (pve: the geometry is a multilinestring!)\n",
    "            poly_coordinates = list()\n",
    "            for line in geometry.geoms:\n",
    "                if not poly_coordinates:\n",
    "                    poly_coordinates = [*list(line.coords)]\n",
    "                    remaining_linestrings = list(geometry[1:])\n",
    "                else:\n",
    "                    poly_coordinates, remaining_lines = find_next_linestring(\n",
    "                        poly_coordinates, remaining_linestrings\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            logging.error(\"Exception occured : %s\", e)\n",
    "            # poly_coordinates = list()\n",
    "            # for line in geometry.geoms:\n",
    "            # poly_coordinates = [*poly_coordinates, *list(line.coords)]\n",
    "        try:\n",
    "            polygon_plot = Polygon(poly_coordinates)\n",
    "        except Exception as e:\n",
    "            # logging.warning(f'Failed creating polygon of geometry {geometry} of gml_id {row.gml_id}. Error: {e}')\n",
    "            logging.warning(\"tst multipolygon\")\n",
    "            polygon_plot = MultiPolygon(poly_coordinates)\n",
    "            # polygon_plot = np.NaN\n",
    "            # TODO: code for multilines will do the trick\n",
    "\n",
    "        # import matplotlib.pyplot as plt; plt.plot(*polygon_plot.exterior.xy); plt.show()\n",
    "        return polygon_plot\n",
    "\n",
    "    gdf[\"geometry\"] = gdf.apply(lambda row: get_cadasteral_boundaries(row), axis=1)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def old_find_next_linestring(poly_coordinates, remaining_linestrings):\n",
    "    \"\"\"\n",
    "    NOT USED ANYMORE\n",
    "    The geometry of get_cadasteral_boundaries gives a multilinestring that\n",
    "    is not ordered correctly for a polygon. This functions finds the next\n",
    "    linestring in the sequence and returns the polygon coordinates and\n",
    "    the remaining linestrings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    poly_coordinates : list\n",
    "        containing coordinate points as tuples\n",
    "    remaining_linestrings : list\n",
    "        containing shapely lines\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    poly_coordinates : list\n",
    "        containing coordinate points as tuples, with points of the next line added\n",
    "    remaining_linestrings : list\n",
    "        containing shapely lines, with one line substracted\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    for line in remaining_linestrings:\n",
    "        if poly_coordinates[-1] == tuple(line.coords[0]):\n",
    "            poly_coordinates = [*poly_coordinates, *list(line.coords)[1:]]\n",
    "            remaining_linestrings.pop(counter)\n",
    "            break\n",
    "        if poly_coordinates[-1] == tuple(line.coords[-1]):\n",
    "            reverse_linestring = list(line.coords)\n",
    "            reverse_linestring.reverse()\n",
    "            poly_coordinates = [*poly_coordinates, *reverse_linestring[1:]]\n",
    "            remaining_linestrings.pop(counter)\n",
    "            break\n",
    "        counter += 1\n",
    "    return poly_coordinates, remaining_linestrings\n",
    "\n",
    "\n",
    "def old_merge_lines(gdf):\n",
    "    \"\"\"\n",
    "    NOT USED\n",
    "    Receives a collection of geometries of the left part and the right\n",
    "    part of a plot border. In de kadastral plot export these geometries consist\n",
    "    of shapely.geometry.LineString and shapely.geometry.MultiLineString.\n",
    "    All separate linestrings are merged to one linestring if possible.\n",
    "    Otherwise a multilinestring will be created.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf: geodataframe\n",
    "        The geodataframe with the linestring parts for the left and right part of the borders\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    final_linestring: shapely linestring or multilinestring\n",
    "        The final result is mostly a linestring but in some situations a multiline\n",
    "        string is returned.\n",
    "    \"\"\"\n",
    "    # print(shapely.ops.linemerge(gdf_left))\n",
    "    result = []\n",
    "    for geom in gdf[\"geometry\"]:\n",
    "        result.append(geom)\n",
    "    final_linestring = shapely.ops.linemerge(result)\n",
    "    return final_linestring\n",
    "\n",
    "\n",
    "def old_create_plots(gdf_plots, gdf_borders, polygon):\n",
    "    \"\"\"\n",
    "    NOT USED ANYMORE, THERE WERE MANY PROBLEMS WITH THE PLOTS. THEREFORE I WANTED TO\n",
    "    KEEP THIS CODE AT HAND. KAN BE REMOVED LATER IF THE CODE APPEARS TO BE STABLE.\n",
    "\n",
    "    Comment PVE: this code is more simple and straightforward and causes no warnings. Is\n",
    "    a bit slower, but made quicker to select only linestring within the polygon.\n",
    "    Lisse takes 5 minutes calculation time.\n",
    "\n",
    "\n",
    "    TODO: I this method is used the gdf_grens should be used to obtain the administrative\n",
    "    kadastral info. In this function this is not implemented. The function is meant to get\n",
    "    better understanding of the downloaded gml.\n",
    "\n",
    "    Receives the download from the kadastral website with borders of the plots. All borders are\n",
    "    shapely.geometry.LineString.\n",
    "    All linestrings are merged, for every merged linestring a check is done if it is a closed\n",
    "    linestring. If so, a polygon is created. The polygon is stored with the plot id in a row of\n",
    "    a geodataframe.\n",
    "    In case of a multilinestring, a check is done on each individual linestring if it is closed.\n",
    "    If so, a Polygon is created. All resulting polygons are then merged into a multipolygon and\n",
    "    added to the final result.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf_borders: geodataframe\n",
    "        The geodataframe with the borders for all plots as downloaded from the kadastral website.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gdf_plots: geodataframe\n",
    "        A dataframe with two columns:\n",
    "        - perceelID: the Id of the plot\n",
    "        - geometry: the geometry of the plot as Polygon\n",
    "    \"\"\"\n",
    "    # # create geodataframe for the final result\n",
    "    gdf_result = gpd.GeoDataFrame(\n",
    "        columns=[\"gml_id\", \"geometry\"], geometry=\"geometry\", crs=\"epsg:28992\"\n",
    "    )\n",
    "    polygon = polygon.buffer(10)\n",
    "    # gdf_plots = gdf_plots.loc[gdf_plots.within(polygon)].copy()\n",
    "    # gdf_plots = gdf_plots.rename(columns={'geometry': 'point_geom'})\n",
    "    gdf_plots = gdf_plots.drop(columns=[\"geometry\"])\n",
    "    # loop through alle unique plot id's. Some id's only occur in one of these columns.\n",
    "    for id in gdf_plots[\"gml_id\"]:\n",
    "        # select all lineparts for the left and the right, merge these to one geometry\n",
    "        collection = gdf_borders.loc[\n",
    "            (gdf_borders[\"perceelLinks|gml_id\"] == id)\n",
    "            | (gdf_borders[\"perceelRechts|gml_id\"] == id)\n",
    "        ][[\"geometry\"]]\n",
    "        ls = merge_lines(collection)\n",
    "        if ls.within(polygon):\n",
    "            geom = None\n",
    "            if ls.is_closed:\n",
    "                # if the geometry is closed, create a polygon\n",
    "                geom = Polygon(ls.coords)\n",
    "                # gdf_plots.loc[gdf_plots['gml_id'] == id][['geometry']] = geom\n",
    "            elif type(ls) is MultiLineString:\n",
    "                # In case of a multilinestring check each individual linestring\n",
    "                # if it is closed and if yes, create a polygon with holes.\n",
    "                # Note: first a Multipolygon was created but that's not right. It will\n",
    "                # lead to errors.\n",
    "                # It appears that linestring occur that are not closed. This makes the\n",
    "                # code a bit more complicated but a check is_closed is needed!\n",
    "                outerbound = None\n",
    "                holes = []\n",
    "                for hole in ls:\n",
    "                    # check if the hole is closed\n",
    "                    if id == 23150428970000:\n",
    "                        print(f\"\\n hole: {hole} \\n {hole.is_closed}\\n\\n\")\n",
    "                    if hole.is_closed:\n",
    "                        if outerbound is None:\n",
    "                            outerbound = hole\n",
    "                        else:\n",
    "                            if id == 23150428970000:\n",
    "                                print(f\"outerbound = {outerbound}\\n hole = {hole}\")\n",
    "\n",
    "                            if Polygon(outerbound).contains(hole):\n",
    "                                holes.append(hole.coords)\n",
    "                            else:\n",
    "                                # if not then the hole must be the outerbound\n",
    "                                holes.append(outerbound.coords)\n",
    "                                outerbound = hole\n",
    "\n",
    "                geom = Polygon(outerbound.coords, holes)\n",
    "                # gdf_plots.loc[gdf_plots['gml_id'] == id][['geometry']] = gpd.GeoSeries([geom])\n",
    "            row = {\"gml_id\": id, \"geometry\": geom}\n",
    "            gdf_result = gdf_result.append(row, ignore_index=True)\n",
    "    gdf_result = gdf_result.merge(gdf_plots, on=\"gml_id\", suffixes=[\"\", \"_perc\"])\n",
    "    test = Polygon\n",
    "    test.interiors\n",
    "    return gdf_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataportaal > io > pdok_wfs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This file contains functions to read data from\n",
    "pdok wfs for administrative areas (bestuurlijke gebieden)\n",
    "and the Administration Adresses and Buildings (BAG -\n",
    "Basisregistratie Adressen en Gebouwen).\n",
    "\n",
    "\n",
    "Ter info:\n",
    "Per 1 juli 2021 is de opvolger van Bestuurlijke grenzen live gezet namelijk de\n",
    "Bestuurlijke gebieden.\n",
    "De dataset Bestuurlijke grenzen wordt niet meer bijgewerkt;\n",
    "de WMS en WFS services blijven tot eind 2021 live, daarna worden deze\n",
    "definitief uit productie genomen.\n",
    "De downloads blijven wel beschikbaar maar zullen per jaar verdwijnen.\n",
    "Nieuwe url:\n",
    "https://service.pdok.nl/kadaster/bestuurlijkegebieden/wfs\n",
    "                                       /v1_0?request=GetCapabilities&service=WFS\n",
    "\"\"\"\n",
    "\n",
    "BEST_GR_WFS = \"https://service.pdok.nl/kadaster/bestuurlijkegebieden/wfs/v1_0\"\n",
    "GEMEENTEN_LAYER = \"bestuurlijkegebieden:Gemeentegebied\"\n",
    "\n",
    "BAG_WFS = \"https://service.pdok.nl/lv/bag/wfs/v2_0\"\n",
    "# pdok_bag_url_wfs = \"https://geodata.nationaalgeoregister.nl/bag/wfs/v1_1\"\n",
    "\n",
    "\n",
    "def get_wktpolygon_municipality(name):\n",
    "    \"\"\"\n",
    "    Returns wkt geometry for a given municipality from the pdok\n",
    "    WFS service 'bestuurlijkegrenzen'.\n",
    "    The filter for the name is applied directly to the WFS request.\n",
    "\n",
    "    Function is obsolete, bhbox is more usefull.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: str\n",
    "        The name of the municipality\n",
    "\n",
    "    Returns: array of str\n",
    "        The strings contain the wkt Polygon(s) for the given municipality.\n",
    "    \"\"\"\n",
    "    version = \"2.0.0\"\n",
    "    srs = \"EPSG:28992\"\n",
    "    outputformat = \"json\"\n",
    "\n",
    "    # Apply the filter to the wfs request\n",
    "    filter1 = fes.PropertyIsEqualTo(\"naam\", name)\n",
    "    f_r = fes.FilterRequest()\n",
    "    filter_fes = f_r.setConstraint(filter1, tostring=True)\n",
    "\n",
    "    # Set all parameters for the wfs request\n",
    "    params = dict(\n",
    "        service=\"WFS\",\n",
    "        version=version,\n",
    "        request=\"GetFeature\",\n",
    "        typeName=GEMEENTEN_LAYER,\n",
    "        srsname=srs,\n",
    "        filter=filter_fes,\n",
    "        outputFormat=outputformat,\n",
    "    )\n",
    "    # Parse the URL with parameters\n",
    "    req_url = requests.Request(\"GET\", BEST_GR_WFS, params=params).prepare().url\n",
    "\n",
    "    # Read data from URL, gives ERROR: Could not resolve host: geo,\n",
    "    # however it does return the correct data\n",
    "    data = gpd.read_file(req_url)\n",
    "    polygon = unary_union(data[\"geometry\"])\n",
    "\n",
    "    # polygon_list = []\n",
    "    # polygons = json.\\\n",
    "    #     loads(data.to_json())['features'][0]['geometry']['coordinates']\n",
    "    # for polygon in polygons:\n",
    "    #     wktpolygon = create_wkt_polygon(polygon)\n",
    "    #     polygon_list.append(wktpolygon)\n",
    "    return polygon\n",
    "\n",
    "\n",
    "def get_wkt_bbox_municipality(mun):\n",
    "    \"\"\"\n",
    "    Returns wkt geometry of the bbox for a given municipality from\n",
    "    the pdok WFS service 'bestuurlijkegrenzen'.\n",
    "    The filter for the name is applied directly to the WFS request.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: str\n",
    "        The name of the municipality\n",
    "\n",
    "    Returns: array of str\n",
    "        The string contain the wkt polygon of the bbox of the\n",
    "        polygon(s) for the given municipality.\n",
    "    \"\"\"\n",
    "    version = \"2.0.0\"\n",
    "    srs = \"EPSG:28992\"\n",
    "    outputformat = \"json\"\n",
    "\n",
    "    # Apply the filter to the WFS request\n",
    "    filter1 = fes.PropertyIsEqualTo(\"naam\", mun)\n",
    "    f_r = fes.FilterRequest()\n",
    "    filter_fes = f_r.setConstraint(filter1, tostring=True)\n",
    "\n",
    "    # Fill the parameters of the WFS request\n",
    "    params = dict(\n",
    "        service=\"WFS\",\n",
    "        version=version,\n",
    "        request=\"GetFeature\",\n",
    "        typeName=GEMEENTEN_LAYER,\n",
    "        srsname=srs,\n",
    "        filter=filter_fes,\n",
    "        outputFormat=outputformat,\n",
    "    )\n",
    "    # Parse the URL with parameters\n",
    "    req_url = requests.Request(\"GET\", BEST_GR_WFS, params=params).prepare().url\n",
    "    # print(req_url)\n",
    "\n",
    "    # Read data from URL, gives ERROR: Could not resolve host: geo,\n",
    "    # however it does return the correct data\n",
    "    data = gpd.read_file(req_url)\n",
    "    bbox = data.geometry.bounds\n",
    "    xmin = bbox.iloc[0][\"minx\"]\n",
    "    ymin = bbox.iloc[0][\"miny\"]\n",
    "    xmax = bbox.iloc[0][\"maxx\"]\n",
    "    ymax = bbox.iloc[0][\"maxy\"]\n",
    "\n",
    "    bbox_municipality = (\n",
    "        f\"POLYGON(({xmin} {ymin},{xmin} {ymax},{xmax} {ymax},\"\n",
    "        + f\"{xmax} {ymin},{xmin} {ymin}))\"\n",
    "    )\n",
    "    return bbox_municipality\n",
    "\n",
    "\n",
    "def create_wkt_polygon(polygon):\n",
    "    \"\"\"\n",
    "    Receives polygon coordinates in json format and returns a string\n",
    "    containing a wkt polygon.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    polygon: json\n",
    "        A list of coordinates of a polygon.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    wktpolygon: str\n",
    "        A str containing a wkt polygon with all coordinates.\n",
    "    \"\"\"\n",
    "    wktpolygon = \"POLYGON((\"\n",
    "    for coordinate in polygon[0]:\n",
    "        wktpolygon += str(coordinate[0]) + \" \" + str(coordinate[1]) + \",\"\n",
    "    wktpolygon = wktpolygon[:-1] + \"))\"\n",
    "    return wktpolygon\n",
    "\n",
    "\n",
    "def get_contour_municipalities(municipality, data_folder, use_cache):\n",
    "    \"\"\"Get a GeoDataFrame with municipality contours as polygons\n",
    "    from a list of municipality strings\n",
    "\n",
    "    Retrieves information via the PDOK WFS service. Source:\n",
    "    https://www.pdok.nl/introductie/-/article/bestuurlijke-grenzen\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    municipalities : list\n",
    "        Containing municipalities as strings, eg: ['Leiden', 'Leiderdorp']\n",
    "    data_folder : str\n",
    "        Folder to write cache data to\n",
    "    use_cache: bool\n",
    "        If true, it will be tried to read data from disk. If in that\n",
    "        case no data is found, it will be downloaded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        Containing the geometries of municipalities\n",
    "    \"\"\"\n",
    "\n",
    "    contour_filename = Path(data_folder) / f\"contour_municipality_{municipality}.json\"\n",
    "\n",
    "    # if we already have the file we will use that one\n",
    "    # unless we ask for an update\n",
    "    if not contour_filename.exists() or not use_cache:\n",
    "        logging.debug(\"Start downloading municipality: %s\", municipality)\n",
    "\n",
    "        # Apply the filter to the wfs request\n",
    "        filter1 = fes.PropertyIsEqualTo(\"naam\", municipality)\n",
    "        f_r = fes.FilterRequest()\n",
    "        filter_fes = f_r.setConstraint(filter1, tostring=True)\n",
    "\n",
    "        params = dict(\n",
    "            service=\"WFS\",\n",
    "            request=\"GetFeature\",\n",
    "            typeName=GEMEENTEN_LAYER,\n",
    "            srsname=\"EPSG:28992\",\n",
    "            filter=filter_fes,\n",
    "            version=\"2.0.0\",\n",
    "            outputFormat=\"json\",\n",
    "        )\n",
    "        # Parse the URL with parameters\n",
    "        req_url = requests.Request(\"GET\", BEST_GR_WFS, params=params).prepare().url\n",
    "\n",
    "        # Download as a file\n",
    "        response = requests.get(req_url)\n",
    "        with open(contour_filename, \"w\") as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "    # Read data from URL\n",
    "    gdf = gpd.read_file(contour_filename)\n",
    "    # gdf_filter = gdf[gdf['naam'] == municipality]\n",
    "    # alternative method for selecting multiple municipalities:\n",
    "    # gdf_filter = gpd.GeoDataFrame()\n",
    "    # for municipality in municipalities:\n",
    "    #     gdf_filter = gdf_filter.\\\n",
    "    #           append(gdf[gdf['gemeentenaam'] == municipality])\n",
    "    polygon_municipalities = unary_union(gdf[\"geometry\"])\n",
    "    return gdf, polygon_municipalities\n",
    "\n",
    "\n",
    "def check_allplots_on_bag(gdf):\n",
    "    \"\"\"\n",
    "    Receives a geodataframe with plots, and checks for each plot on bag wfs\n",
    "    if it contains address information. All found addresses are stored in a\n",
    "    dataframe that is returned.\n",
    "\n",
    "    TODO: Find out why sometimes the service crashes on a timeout. Maybe\n",
    "    because requests are to quick after each other??\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf: Geodataframe\n",
    "        A geodataframe that contains plot data, read from the kadaster webservice.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    eindresultaat: geodataframe\n",
    "        A geodataframe with the plot polygons and all addresses for each plot.\n",
    "    \"\"\"\n",
    "    tot_rows = len(gdf)\n",
    "    eindresultaat = None\n",
    "    for count in range(0, tot_rows - 1):\n",
    "        row = gdf.iloc[count]\n",
    "        try:\n",
    "            check_result = check_plot_on_bag(row)\n",
    "        except:\n",
    "            logging.error(\n",
    "                \"Error in plot with localID: %s. No check performed\", row[\"lokaalID\"]\n",
    "            )\n",
    "\n",
    "        if check_result is not None:\n",
    "            if eindresultaat is None:\n",
    "                eindresultaat = check_result.copy()\n",
    "            else:\n",
    "                eindresultaat = gpd.GeoDataFrame(\n",
    "                    pd.concat([eindresultaat, check_result], ignore_index=True)\n",
    "                )\n",
    "    return eindresultaat.reset_index()\n",
    "\n",
    "\n",
    "def check_plot_on_bag(plot_row):\n",
    "    \"\"\"\n",
    "    Receives a row from the plots dataframe and checks if the geometry contains\n",
    "    bag locatieons with function 'get_bag_locations'.\n",
    "    - First checks if an addresslocation is found, if not:\n",
    "    - Checks if an 'ligplaats' is found, if not:\n",
    "    - Checks if an 'standplaats' is found.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    plot_row: dataframe.row\n",
    "        The row of the dataframe with the plot that should be checked.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gdf_bag_result: dataframe or None if nothing found\n",
    "        A dataframe that contains all addressess found. Each row in the dataframe contains\n",
    "        the geometry of the plot.\n",
    "    \"\"\"\n",
    "    # In kadaster data some very long prefixes occur, this is one of them:\n",
    "    c_gemcode = (\n",
    "        \"kadastraleAanduiding|TypeKadastraleAanduiding|\"\n",
    "        + \"aKRKadastraleGemeenteCode|AKRKadastraleGemeenteCode|waarde\"\n",
    "    )\n",
    "    c_perceelnr = \"perceelnummer\"\n",
    "    c_letter = \"sectie\"\n",
    "\n",
    "    kadasterid = (\n",
    "        plot_row[c_gemcode]\n",
    "        + \"-\"\n",
    "        + plot_row[c_letter]\n",
    "        + \"-\"\n",
    "        + str(plot_row[c_perceelnr])\n",
    "    )\n",
    "    kadaster_locaalid = plot_row[\"lokaalID\"]\n",
    "\n",
    "    geometry = plot_row.geometry\n",
    "    gdf_bag_result = get_bag_locations(geometry)\n",
    "\n",
    "    if gdf_bag_result is None:\n",
    "        gdf_bag_result = get_bag_locations(geometry, \"ligplaats\")\n",
    "        if gdf_bag_result is None:\n",
    "            gdf_bag_result = get_bag_locations(geometry, \"standplaats\")\n",
    "            if gdf_bag_result is not None:\n",
    "                gdf_bag_result[\"gebruiksdoel\"] = \"Standplaats\"\n",
    "        else:\n",
    "            gdf_bag_result[\"gebruiksdoel\"] = \"Ligplaats\"\n",
    "\n",
    "    if gdf_bag_result is not None:\n",
    "        gdf_bag_result = gdf_bag_result.set_crs(\"EPSG:28992\")\n",
    "        gdf_bag_result = gdf_bag_result[\n",
    "            [\n",
    "                \"openbare_ruimte\",\n",
    "                \"huisnummer\",\n",
    "                \"huisletter\",\n",
    "                \"toevoeging\",\n",
    "                \"postcode\",\n",
    "                \"woonplaats\",\n",
    "                \"gebruiksdoel\",\n",
    "                \"geometry\",\n",
    "            ]\n",
    "        ]\n",
    "        gdf_bag_result[\"geometry\"] = geometry\n",
    "        gdf_bag_result[\"kadaster\"] = kadasterid\n",
    "        gdf_bag_result[\"kadaster_locaalid\"] = kadaster_locaalid\n",
    "\n",
    "    return gdf_bag_result\n",
    "\n",
    "\n",
    "def create_gml_multipolygon(geometry):\n",
    "    \"\"\"\n",
    "    Receives a shapely MultiPolygon geometry and converts it to\n",
    "    a gml Multipolygon that can serve as a filter in a WFS request.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    geometry: shapely.geometry.Multipolygon\n",
    "        The multipolygon to convert\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gml_multipolygon: str\n",
    "        A gml representation of the multipolygon\n",
    "    \"\"\"\n",
    "    polygon_members = \"\"\n",
    "\n",
    "    for polygon in list(geometry.geoms):\n",
    "        gml_polygon = create_gml_polygon(polygon, False)\n",
    "        polygon_members += f\"\"\"\n",
    "        <gml:polygonMember>\n",
    "            {gml_polygon}\n",
    "        </gml:polygonMember>\n",
    "        \"\"\"\n",
    "\n",
    "    gml_multipolygon = f\"\"\"<gml:MultiPolygon gml:id=\"filter\" srsName=\"urn:ogc:def:crs:EPSG::28992\">\n",
    "                {polygon_members}\n",
    "\t        </gml:MultiPolygon>\"\"\"\n",
    "    return gml_multipolygon\n",
    "\n",
    "\n",
    "def create_gml_linearing(coords):\n",
    "    \"\"\"\n",
    "    Receives coordinates from a shapely geometry and create an geml linear ring from them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coords: array\n",
    "        The coordinates of the geometry that should be converted to a gml linearRing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    linearRing: str\n",
    "        The gml string with the coordinates as linearRing\n",
    "    \"\"\"\n",
    "\n",
    "    coordinates = \"\"\n",
    "    for coordinate in coords:\n",
    "        coordinates += str(coordinate[0]) + \" \" + str(coordinate[1]) + \" \"\n",
    "\n",
    "    coordinates = coordinates[:-1]\n",
    "    linearRing = f\"\"\"<gml:LinearRing><gml:posList srsDimension=\"2\">\n",
    "    {coordinates}</gml:posList></gml:LinearRing>\"\"\"\n",
    "    return linearRing\n",
    "\n",
    "\n",
    "def create_gml_interiors(interiors):\n",
    "    \"\"\"\n",
    "    Receives a shapely geometry.interiors and converts it to\n",
    "    a gml interiors that can serve as a filter in a WFS request.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    interiors: geometry.interiors\n",
    "        The interiors\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    interiors: str\n",
    "        A gml representation of the interiors (holes)\n",
    "    \"\"\"\n",
    "    interiors_gml = \"\"\n",
    "\n",
    "    for interior in interiors:\n",
    "        linRing = create_gml_linearing(interior.coords)\n",
    "        interiors_gml += f\"\"\"\n",
    "        <gml:interior>{linRing}</gml:interior>\n",
    "        \"\"\"\n",
    "    return interiors_gml\n",
    "\n",
    "\n",
    "def create_gml_polygon(geometry, filter):\n",
    "    \"\"\"\n",
    "    Receives a shapely Polygon geometry and converts this to\n",
    "    a gml polygon. It can serve as part of a MultiPolygon or as\n",
    "    a filter itself.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    geometry: shapely.geometry.Polygon\n",
    "        The polygon to convert\n",
    "    filter: bool\n",
    "        If filter is True, an attribute filter is added.\n",
    "        This is needed if the polygon itself is the filter.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gml_polygon: str\n",
    "        A gml presentation of the polygon\n",
    "\n",
    "    \"\"\"\n",
    "    extRing = create_gml_linearing(geometry.exterior.coords)\n",
    "    interiors = create_gml_interiors(geometry.interiors)\n",
    "\n",
    "    filter_attribute = \"\"\n",
    "    if filter:\n",
    "        filter_attribute = 'gml:id=\"filter\"'\n",
    "\n",
    "    gml_polygon = f\"\"\"\n",
    "        <gml:Polygon {filter_attribute} srsName=\"urn:ogc:def:crs:EPSG::28992\">\n",
    "        <gml:exterior>\n",
    "        {extRing}\n",
    "        </gml:exterior>\n",
    "        {interiors}\n",
    "        </gml:Polygon>\n",
    "        \"\"\"\n",
    "    return gml_polygon\n",
    "\n",
    "\n",
    "def get_bag_locations(geometry, typename=\"verblijfsobject\"):\n",
    "    \"\"\"\n",
    "    Retrieves from the pdok WFS service for bag a geodataframe with bag\n",
    "    locations that meet two requirements:\n",
    "    - Lie within a given polygon (geometry)\n",
    "    - AND are in use (status = 'Verblijfsobject in gebruik' or\n",
    "                      status = 'Plaats aangewezen')\n",
    "    Uses a post request.\n",
    "    See: https://www.kadaster.nl/-/beschrijving-bag-wfs\n",
    "\n",
    "    Parameters:\n",
    "    geometry: Polygon\n",
    "        The polygon that filters the bag locations.\n",
    "    typename: str\n",
    "        The typename(s) of the layer for which objects should be returned.\n",
    "        If more names are used, separate them with '&'\n",
    "    \"\"\"\n",
    "    status = \"Verblijfsobject in gebruik\"\n",
    "    if typename != \"verblijfsobject\":\n",
    "        status = \"Plaats aangewezen\"\n",
    "\n",
    "    filter_geometry = create_gml_polygon(geometry, True)\n",
    "\n",
    "    # Intersects vervangen door Within\n",
    "    postheader = f\"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "        <GetFeature xmlns=\"http://www.opengis.net/wfs/2.0\" xmlns:gml=\"http://www.opengis.ne\n",
    "        t/gml/3.2\" service=\"WFS\" version=\"2.0.0\" outputformat=\"json\"\n",
    "        xmlns:xsi=\"http://www.w3.org/2001/XMLSchem\n",
    "        ainstance\" xsi:schemaLocation=\"http://schemas.opengis.net/wfs/2.0/wfs.xsd http://sch\n",
    "        emas.opengis.net/wfs/2.0.0/WFS-transaction.xsd\">\n",
    "        <Query typeNames=\"{typename}\" xmlns:bag=\"http://bag.geonovum.nl\">\n",
    "        <fes:Filter xmlns:fes=\"http://www.opengis.net/fes/2.0\">\n",
    "        <fes:And>\n",
    "        <fes:Within>\n",
    "        <fes:ValueReference>geometrie</fes:ValueReference>\n",
    "        {filter_geometry}\n",
    "        </fes:Within>\n",
    "        <fes:PropertyIsEqualTo>\n",
    "        <fes:PropertyName>status</fes:PropertyName>\n",
    "        <fes:Literal>{status}</fes:Literal>\n",
    "        </fes:PropertyIsEqualTo>\n",
    "        </fes:And>\n",
    "        </fes:Filter>\n",
    "        </Query>\n",
    "        </GetFeature>\n",
    "        \"\"\"\n",
    "    # Step 1 is send the request to the BGT api\n",
    "    req_url = requests.post(\n",
    "        BAG_WFS,\n",
    "        data=postheader,\n",
    "        headers={\"accept\": \"application/xml\", \"Content-Type\": \"application/json\"},\n",
    "    )\n",
    "    gdf = None\n",
    "    if req_url.status_code == 200:  # Status = O.k.\n",
    "        # print(req_url.json())\n",
    "        gdf = gpd.GeoDataFrame.from_features(req_url.json())\n",
    "        if gdf.empty:\n",
    "            gdf = None\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def get_gdf_wfs(url, gdf, layers, sortby, n_cells=30):\n",
    "    \"\"\"\n",
    "    Gets all data from BAG WFS using function 'get_gdf_wfs_loop'\n",
    "\n",
    "    Available layers:\n",
    "    https://www.nationaalgeoregister.nl/geonetwork\n",
    "    /srv/dut/catalog.search#/metadata/1c0dcc64-91aa-4d44-a9e3-54355556f5e7  # nopep8\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : GeoDataFrame\n",
    "        containing the shape of the area to load through the WFS service\n",
    "    data_folder : str\n",
    "        folder for cache data\n",
    "    n_cells : int, optional\n",
    "        amount of cells to divide the dataframe in, by default 30\n",
    "    cache_data : bool, optional\n",
    "        Caching gdf to data_folder if True, by default False\n",
    "    \"\"\"\n",
    "    gdf_total = gpd.GeoDataFrame()\n",
    "    for layer in layers:\n",
    "        gdf_singlelayer = get_gdf_wfs_loop(url, gdf, layer, sortby, n_cells=n_cells)\n",
    "        gdf_singlelayer = gdf_singlelayer.drop_duplicates()\n",
    "        gdf_singlelayer[\"layer\"] = layer\n",
    "        gdf_total = gdf_total.append(gdf_singlelayer)\n",
    "        gdf_total = gdf_total.drop_duplicates(subset=sortby)\n",
    "    gdf_total = gdf_total.reset_index(drop=True)\n",
    "    return gdf_total\n",
    "\n",
    "\n",
    "def get_gdf_wfs_loop(url, gdf, layer, sortby, n_cells=30):\n",
    "    \"\"\"\n",
    "    Read all BAG data for a specified layer from the pdok BAG WFS service\n",
    "    that can be found in the bounds of a given geodataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        The base url of the bag wfs service.\n",
    "    gdf: geodataframe\n",
    "        The geodataframe of which the bounds are used to limit the request.\n",
    "    sortby:\n",
    "\n",
    "    \"\"\"\n",
    "    # pdok_bag_url_wfs = \"https://geodata.nationaalgeoregister.nl/bag/wfs/v1_1\"\n",
    "    type_name = layer\n",
    "    coordinate_sys = \"EPSG:28992\"  # 'EPSG:4326'\n",
    "    count = \"1000\"\n",
    "\n",
    "    grid_cells = get_grid_cells_gdf(gdf, n_cells)\n",
    "    # select only the cells in the municipality geometry\n",
    "    grid_cells = grid_cells[~grid_cells.geometry.disjoint(gdf.geometry.unary_union)]\n",
    "    # Uncomment to check the area to download:\n",
    "    # fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    # grid_cells.plot(ax=ax)\n",
    "    # gdf.plot(ax=ax, alpha=0.9, color=\"pink\")\n",
    "\n",
    "    # this used to be lambda x: shapely.wkt.dumps(x)\n",
    "    grid_geometry_strings = grid_cells.geometry.apply(shapely.wkt.dumps)\n",
    "    # drop the 'POLYGON ((...))' part of the geometry string to comply with the WFS filter\n",
    "    grid_geometry_strings = grid_geometry_strings.apply(lambda x: x[10:-2])\n",
    "    logging.info(\n",
    "        \"-> Start loop for layer %s and merging %i cells\",\n",
    "        layer,\n",
    "        len(grid_geometry_strings),\n",
    "    )\n",
    "\n",
    "    gdf_bag_total = gpd.GeoDataFrame()\n",
    "    counter = 0\n",
    "    for cell in grid_geometry_strings:\n",
    "        counter += 1\n",
    "        logging.info(\n",
    "            \"--> Processing cell number %i/%i\", counter, len(grid_geometry_strings)\n",
    "        )\n",
    "        # polygon_coordinates = '89786 460729,89786 461307,98560 461307,98560 460729,89786 460729'\n",
    "        filter_str = create_ogc_polygon_filter(cell)\n",
    "        params = dict(\n",
    "            service=\"WFS\",\n",
    "            request=\"GetFeature\",\n",
    "            typeName=type_name,\n",
    "            srsname=coordinate_sys,\n",
    "            version=\"2.0.0\",\n",
    "            filter=filter_str,\n",
    "        )\n",
    "        gdf = gdf_wfs_appended(url, params, sortby, count)\n",
    "        gdf_bag_total = gdf_bag_total.append(gdf)\n",
    "    gdf_bag_total = gdf_bag_total.reset_index(drop=True)\n",
    "    return gdf_bag_total\n",
    "\n",
    "\n",
    "def get_grid_cells_gdf(gdf, n_cells):\n",
    "    \"\"\"\n",
    "    Receives a geodataframe and desired number of cells that should fit (in x\n",
    "    direction) in the bounds of the geodataframe.\n",
    "    Calculates the xmin, xmax, ymin and ymax of each cell so that all cells\n",
    "    will fit in the bounds of the given geodataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf: geodataframe\n",
    "        The given geodataframe of which the bounds are used to calculate the\n",
    "        cells.\n",
    "    n_cells: int\n",
    "        The desired number of cells that should fit in the dataframe bounds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cells: geodataframe\n",
    "        A geodataframe that contains a square cell in each row with xmin, xmax, ymin, ymax.\n",
    "    \"\"\"\n",
    "    # total area for the grid\n",
    "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "    # how many cells across and down\n",
    "    cell_size = (xmax - xmin) / n_cells\n",
    "    # projection of the grid\n",
    "    crs = gdf.crs\n",
    "    # \"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181\n",
    "    # +b=6371007.181 +units=m +no_defs\"\n",
    "    # create the cells in a loop\n",
    "    grid_cells = []\n",
    "    for x_0 in np.arange(xmin, xmax + cell_size, cell_size):\n",
    "        for y_0 in np.arange(ymin, ymax + cell_size, cell_size):\n",
    "            # bounds\n",
    "            x_1 = x_0 - cell_size\n",
    "            y_1 = y_0 + cell_size\n",
    "            grid_cells.append(shapely.geometry.box(x_0, y_0, x_1, y_1))\n",
    "    cells = gpd.GeoDataFrame(grid_cells, columns=[\"geometry\"], crs=crs)\n",
    "    return cells\n",
    "\n",
    "\n",
    "def gdf_wfs_appended(url, params, sortby, count):\n",
    "    \"\"\"Load all WFS features into one GeoDataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    total_features : int\n",
    "        Total features in a WFS request\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        Containing records equal to total_features\n",
    "    \"\"\"\n",
    "    startindex = 0\n",
    "    total_features = request_count(url, params)\n",
    "    page_indexes = range(startindex, total_features, int(count))\n",
    "    gdf_final = gpd.GeoDataFrame()\n",
    "    logging.info(\n",
    "        \"---> Downloading and appending %i page(s)\", len(page_indexes)\n",
    "    )  # nopep8\n",
    "    for page_nr in page_indexes:\n",
    "        logging.info(\"---> Downloading startindex %s\", page_nr)\n",
    "        gdf = request_gdf_wfs(url, params, page_nr, sortby, count)\n",
    "        gdf_final = gdf_final.append(gdf)\n",
    "    return gdf_final.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def request_count(url, params):\n",
    "    \"\"\"Counts the amount of features in a WFS request\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        query parameter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        amount of features returned by query\n",
    "    \"\"\"\n",
    "    parameters = merge_dicts(params, dict(resulttype=\"hits\"))\n",
    "    req_url = requests.Request(\"GET\", url, params=parameters).prepare().url\n",
    "    resp = requests.get(req_url)\n",
    "    xml_str = resp.content.decode(\"utf-8\")\n",
    "    xml = ET.fromstring(xml_str)\n",
    "    numbers_matched = int(xml.attrib[\"numberMatched\"])\n",
    "    logging.info(\"--> request_count found %i feature in cell\", numbers_matched)\n",
    "    return numbers_matched\n",
    "\n",
    "\n",
    "def request_gdf_wfs(url, params, startindex, sortby, count):\n",
    "    \"\"\"Build a WFS url and read into GeoDataFrame\n",
    "\n",
    "    For more information on the count and startindex parameters, see:\n",
    "    https://geoforum.nl/t/bag-wfs-request-geeft-maar-1000-objecten-maximaal-terug/2405/2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        dictionary of parameters required for the WFS protocol\n",
    "        Example for the bag {'service': 'WFS',\n",
    "                             'request': 'GetFeature',\n",
    "                             'typeName': 'verblijfsobject',\n",
    "                             'srsname': 'EPSG:28992',\n",
    "                             'version': '2.0.0',\n",
    "                             'filter': <a geospatial query in ogc format\n",
    "                                       see function create_ogc_polygon_filter>,\n",
    "                             'sortby': 'identificatie',\n",
    "                             'count': '1000',\n",
    "                             'startindex': '2500'}\n",
    "    count: str or int\n",
    "        Number of features that one WFS request returns\n",
    "        The WFS service has this usually capped at 1000\n",
    "    startindex : str or int\n",
    "        Number of the first returned feature in the request\n",
    "    sortby:\n",
    "        Column to sort, must be in the requested layer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        containing features equal to count\n",
    "    \"\"\"\n",
    "    if int(startindex) > 50000:\n",
    "        logging.warning(\n",
    "            \"Most WFS services have startindexlimit of not more than 50000,\\\n",
    " make your geometry filter smaller\"\n",
    "        )\n",
    "    startindex = str(startindex)\n",
    "    parameters = merge_dicts(\n",
    "        params,\n",
    "        dict(outputFormat=\"json\", sortby=sortby, count=count, startindex=startindex),\n",
    "    )\n",
    "    req_url = requests.Request(\"GET\", url, params=parameters).prepare().url\n",
    "    gdf = gpd.read_file(req_url)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def create_ogc_polygon_filter(polygon_coordinates):\n",
    "    \"\"\"create a polygon filter for a WFS request, according to the ogc standard\n",
    "\n",
    "    filter reference:\n",
    "    https://docs.geoserver.org/latest/en/user/filter/filter_reference.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    polygon_coordinates : str\n",
    "        Str with enclosed points in the coordinate system requested, eg:\n",
    "        '89800 460800,89800 460900,89900 460900,89900 460800,89900 460900'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Ogc filter as a str, in xml format\n",
    "    \"\"\"\n",
    "    el_filter = ET.Element(\"ogc:Filter\")\n",
    "    el_not = ET.SubElement(el_filter, \"Not\")\n",
    "    el_disjoint = ET.SubElement(el_not, \"Disjoint\")\n",
    "    el_pn = ET.SubElement(el_disjoint, \"PropertyName\")\n",
    "    el_pn.text = \"Geometry\"\n",
    "    el_p = ET.SubElement(el_disjoint, \"gml:Polygon\")\n",
    "    el_ob = ET.SubElement(el_p, \"gml:outerBoundaryIs\")\n",
    "    el_ls = ET.SubElement(el_ob, \"gml:LinearRing\")\n",
    "    el_pl = ET.SubElement(el_ls, \"gml:posList\")\n",
    "    el_pl.text = polygon_coordinates\n",
    "    filter_str = ET.tostring(el_filter, encoding=\"utf8\", method=\"xml\").decode()[38:]\n",
    "    logging.debug(\"Ogc filter string is %s\", filter_str)\n",
    "    return filter_str\n",
    "\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    \"\"\"Merge two dictionaries together\"\"\"\n",
    "    dict_merged = dict1.copy()  # start with keys and values of x\n",
    "    dict_merged.update(dict2)  # modifies z with keys and values of y\n",
    "    return dict_merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataportaal > io > bgt_api_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bgt_features_mun(\n",
    "    municipality, outputpath=\"\", cache_data=True, features='[\"wegdeel\", \"waterdeel\"]'\n",
    "):\n",
    "    \"\"\"\n",
    "    Gets feature layers from bgt download for a\n",
    "    given municipality. Saves the data as parquet to a given output path.\n",
    "    Uses function 'get_wkt_bbox_municipality(mun)' from pdok_wfs to get the\n",
    "    bounding box of the municipality. After that calls function\n",
    "    'get_bgt_features_poly' to actually retrieve the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    municipality: str\n",
    "        The name of the municipality.\n",
    "\n",
    "    outputpath: str\n",
    "        The path to which the data should be written as parquet\n",
    "\n",
    "    cache_data: bool, optional\n",
    "        If True the data are saved to the outputpath as parquet\n",
    "        Default value = True\n",
    "\n",
    "    features: str, optional\n",
    "        A string containing the names of the features to get.\n",
    "        Default = '[\"wegdeel\",\"waterdeel\"]'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: dict of geodataframes\n",
    "        Contains for each feature layer a geodataframe\n",
    "    \"\"\"\n",
    "    wkt_bbox = get_wkt_bbox_municipality(municipality)\n",
    "    data = get_bgt_features_poly(wkt_bbox, outputpath, cache_data, features)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_bgt_features_poly(\n",
    "    selectpolygon, outputpath=\"\", cache_data=True, features='[\"wegdeel\", \"waterdeel\"]'\n",
    "):\n",
    "    \"\"\"\n",
    "    Gets feature layers from bgt download for a\n",
    "    given wkt polygon. Saves the data as parquet to a given output path if\n",
    "    cache_data=True.\n",
    "    The raw data is saved to a internal temp directory, so not directly\n",
    "    available.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selectpolygon: str\n",
    "        The polygon in which the features are selected.\n",
    "        In wkt format.\n",
    "\n",
    "    outputpath: str\n",
    "        The path to which the data should be written as parquet\n",
    "\n",
    "    cache_data: bool, optional\n",
    "        If True, the data will be saved to the outputpath as parquet.\n",
    "        Default value = True\n",
    "\n",
    "    features: str, optional\n",
    "        A string containing the names of the features to get.\n",
    "        Default = '[\"wegdeel\",\"waterdeel\"]'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: dict of geodataframes\n",
    "        Contains for each feature layer a geodataframe\n",
    "    \"\"\"\n",
    "    temp_dir = tempfile.TemporaryDirectory()\n",
    "    temp_path = temp_dir.name\n",
    "    zippath = os.path.join(temp_path, \"bgt_extract.zip\")\n",
    "\n",
    "    file_path = download_bgt_data(selectpolygon, zippath, features)\n",
    "    returnvalue = {}\n",
    "\n",
    "    if file_path is not None:\n",
    "        # Retrieve all gml data from the downloaded zip file.\n",
    "        # Convert the citygml to normal gml and read it as gdf.\n",
    "        with zipfile.ZipFile(zippath) as z_file:\n",
    "            z_file.extractall(temp_path)\n",
    "            for file in z_file.infolist():\n",
    "                input_file = file.filename\n",
    "                output_file = \"conv_\" + input_file\n",
    "                gdal.VectorTranslate(\n",
    "                    os.path.join(temp_path, output_file),\n",
    "                    os.path.join(temp_path, input_file),\n",
    "                    options='-f \"gml\" -nlt CONVERT_TO_LINEAR',  # converting curved polygons to lineair to avoid fiona read error\n",
    "                )\n",
    "                # raises error, unsupported type 10 -> curvepolygon\n",
    "                # https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry#Well-known_binary\n",
    "                # solution: https://gis.stackexchange.com/questions/433216/read-xml-with-curvepolygon-with-geopandas-fiona\n",
    "                gdf = gpd.read_file(os.path.join(temp_path, output_file))\n",
    "                if cache_data:\n",
    "                    gdf.to_parquet(\n",
    "                        os.path.join(outputpath, input_file.replace(\"gml\", \"parquet\"))\n",
    "                    )\n",
    "                returnvalue[input_file.replace(\".gml\", \"\")] = gdf\n",
    "    return returnvalue\n",
    "\n",
    "\n",
    "def download_bgt_data(selectpolygon, outputpath, features):\n",
    "    \"\"\"\n",
    "    Downloads bgt features of a given selection polygon (wkt) as\n",
    "    citygml (imgeo) in a zip file to a given outputpath.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    selectpolygon: str\n",
    "        The polygon of the selection (wkt format)\n",
    "\n",
    "    outputpath: str\n",
    "        The path to save the zip file to\n",
    "\n",
    "    features: str\n",
    "        The features to download. This is a string containing an\n",
    "        array with featurenames (e.g.: '[\"wegdeel\", \"waterdeel\"]')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    file_url: str\n",
    "        The path to the donwloaded file, in case of error None is returned.\n",
    "    \"\"\"\n",
    "    file_url = None\n",
    "    # These are the eindpoints for the BGT API\n",
    "    baseurl = \"https://api.pdok.nl\"\n",
    "    requesturi = \"/lv/bgt/download/v1_0/full/custom\"\n",
    "    statusuri = \"/lv/bgt/download/v1_0/full/custom/-id-/status\"\n",
    "\n",
    "    # Here the parameters for the request are prepared.\n",
    "    data = (\n",
    "        '{\"featuretypes\": -features-,'\n",
    "        + '\"format\":\"citygml\",'\n",
    "        + '\"geofilter\": \"-polygon-\"}'\n",
    "    )\n",
    "    data = data.replace(\"-features-\", features)\n",
    "    data = data.replace(\"-polygon-\", selectpolygon)\n",
    "\n",
    "    # Step 1 is send the request to the BGT api\n",
    "    resp_download_id = requests.post(\n",
    "        baseurl + requesturi,\n",
    "        data=data,\n",
    "        headers={\"accept\": \"application/json\", \"Content-Type\": \"application/json\"},\n",
    "    )\n",
    "    # Step 2 is check if the request is accepted and wait until\n",
    "    #        the data are prepared.\n",
    "    if resp_download_id.status_code == 202:\n",
    "        download_id = resp_download_id.json()[\"downloadRequestId\"]\n",
    "        statusurl = baseurl + statusuri.replace(\"-id-\", download_id)\n",
    "        try:\n",
    "            resp_download_url = requests.get(statusurl)\n",
    "            while resp_download_url.json()[\"status\"] != \"COMPLETED\":\n",
    "                logging.info(\n",
    "                    \"Request in progress (%s)\", resp_download_url.json()[\"progress\"]\n",
    "                )\n",
    "                time.sleep(1)\n",
    "                resp_download_url = requests.get(statusurl)\n",
    "        except requests.exceptions.RequestException as ex:\n",
    "            logging.error(\"Error during preparing BGT data. {%s}\", ex)\n",
    "\n",
    "        try:\n",
    "            downloaduri = resp_download_url.json()[\"_links\"][\"download\"][\"href\"]\n",
    "            downloadurl = baseurl + downloaduri\n",
    "            file_url = wget.download(downloadurl, outputpath)\n",
    "\n",
    "        except HTTPException as ex:\n",
    "            logging.error(\"Error requesting BGT data {%s}.\", ex)\n",
    "    else:\n",
    "        logging.info(\"Fout bij versturen aanvraag.\")\n",
    "\n",
    "    return file_url\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # test = get_bgt_features_mun(\"Leiden\", os.path.join(os.getcwd(),'research', 'data'))\n",
    "    gdf_result = gpd.read_parquet(\n",
    "        os.path.join(os.getcwd(), \"research\", \"data\", \"bgt_wegdeel.parquet\")\n",
    "    )\n",
    "    print(gdf_result[\"function\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataportaal > calc > select_plot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This file contains functions that are used to do\n",
    "the 'calculations' which are in fact selections of plots.\n",
    "\"\"\"\n",
    "\n",
    "def create_buffer(gdf, crs=\"EPSG:28992\", bufsize=40):\n",
    "    \"\"\"\n",
    "    Receives a geodataframe, sets the projection and then\n",
    "    creates a buffer around its geometry. All buffered geometries are\n",
    "    unified through a union and returned as one polygon.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf: geodataframe\n",
    "        The geodataframe for which a buffer is created.\n",
    "    crs: crs, optional\n",
    "        The desired crs for the input dataframe.\n",
    "        Default is 'EPSG:28992'\n",
    "    bufsize: float, optional\n",
    "        The buffersize. Default is 40\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    overlay: Polygon\n",
    "        A polygon of the created buffer\n",
    "\n",
    "    \"\"\"\n",
    "    gdf_buf = gdf.copy(deep=True)\n",
    "    gdf_buf = gdf_buf.to_crs(crs)\n",
    "\n",
    "    gdf_buf['geometry'] = gdf_buf.buffer(bufsize)\n",
    "    return gdf_buf.unary_union\n",
    "\n",
    "\n",
    "def select_items_with_spatial_index(source_gdf, mask_gdf):\n",
    "    \"\"\"\n",
    "    Used to limit the number of rows in a geodataframe for an 'expensive'\n",
    "    operations using the spatial index. In this way unary_union etc. will be\n",
    "    limited to less rows so that the calculation will be much quicker.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_gdf: geodataframe\n",
    "        The geodataframe of which the records should be limited.\n",
    "    mask_gdf: geodataframe\n",
    "        The geodataframe that will be used as the mask to limit the\n",
    "        number of records.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    candidate_gdf: geodataframe\n",
    "        The geodataframe with the candidates of which the spatial index\n",
    "        intersects with the mask geometry.\n",
    "    exclude_gdf: geodataframe\n",
    "        The geodataframe with the excluded rows that do not intersect\n",
    "        with the mask geometry.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    source_sindex = source_gdf.sindex\n",
    "    for values in mask_gdf.bounds.values:\n",
    "        bounds = list(values)\n",
    "        matches += list(source_sindex.intersection(bounds))\n",
    "\n",
    "    unique_matches = list(set(matches))\n",
    "\n",
    "    candidate_gdf = source_gdf.loc[unique_matches]\n",
    "    exclude_gdf = source_gdf[~source_gdf.index.isin(candidate_gdf.index)]\n",
    "    return candidate_gdf, exclude_gdf\n",
    "\n",
    "\n",
    "def clip_buffer_select_part(gdf_buffer, mask, gdf_lines):\n",
    "    \"\"\"\n",
    "    Clips a (buffer) polygon dataframe with a given (mask) polygon dataframe\n",
    "    (e.g. water or railway).\n",
    "    Next uses a line dataframe (e.g. sewers) to remove the (buffer) polygons\n",
    "    that do not intersect with a line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf_buffer: geodataframe\n",
    "        The dataframe with the buffer polygons that should be cut.\n",
    "    mask: geodatframe\n",
    "        The dataframe with the mask that will be used to clip the\n",
    "        buffer polygons.\n",
    "    gdf_lines: geodataframe\n",
    "        The dataframe with the (sewer) lines to select the polygons\n",
    "        that intersect with a line.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gdf_cb: geodataframe\n",
    "        Dataframe with clipped buffer polygons that intersect with\n",
    "        a sewer line.\n",
    "    \"\"\"\n",
    "    logging.info(\"Clipping buffergeometry.\")\n",
    "    # mask_poly = gdf_mask.geometry.unary_union\n",
    "    # clipped_buffer = gdf_buffer.cx(mask_poly)\n",
    "    clipped_buffer = gdf_buffer.difference(mask, align=False)\n",
    "\n",
    "    # result = result.loc[~result.contains(gdf_leidingen)]\n",
    "    gdf_cb = gpd.GeoDataFrame()\n",
    "    gdf_cb = gdf_cb.set_geometry(clipped_buffer)\n",
    "    gdf_cb = gdf_cb.set_crs(\"EPSG:28992\")\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Explode clipped buffergeometry.\")\n",
    "        gdf_cb = gdf_cb.explode(column=\"geometry\", ignore_index=True)\n",
    "        # gdf2 = gdf_cb.reset_index().rename(columns={0: 'geometry'})\n",
    "        # gdf_cb = gdf2.merge(gdf_cb.drop('geometry',\n",
    "        #                     axis=1),\n",
    "        #                     left_on='level_0',\n",
    "        #                     right_index=True)\n",
    "        # gdf_cb = gdf_cb.set_index(['level_0',\n",
    "        #                            'level_1']).set_geometry('geometry')\n",
    "    except Exception as e:\n",
    "        logging.error(\"Explode in clipped buffer error: {%s}\", e)\n",
    "\n",
    "    logging.info(\"Skip buffers without sewer/pump.\")\n",
    "    # 'within' 'intersects' 'contain'\n",
    "    gdf_cb = gpd.sjoin(gdf_cb, gdf_lines, 'left', 'intersects')\n",
    "    gdf_cb = gdf_cb.loc[~gdf_cb['index_right'].isna()]\n",
    "    return gdf_cb\n",
    "\n",
    "\n",
    "def select_plots(gdf, overlay):\n",
    "    \"\"\"\n",
    "    Selects plots of a geodataframe with plots from kadaster that are outside a\n",
    "    given overlay-geometry.\n",
    "    The selection is performed through the 'intersect' method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf: geodataframe\n",
    "        The dataframe with the plot geometries.\n",
    "    overlay: Polygon\n",
    "        The geometry used to perform the selection.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gdf: geodataframe\n",
    "        The geodataframe with the final result\n",
    "    \"\"\"\n",
    "    gdf['intersect'] = gdf.geometry.apply(lambda x: x.intersects(overlay))\n",
    "    gdf_outside = gdf.loc[~gdf['intersect']]\n",
    "    # gdf_outside = gdf[gdf.disjoint(overlay)] # Dit doet precies hetzelfde\n",
    "    return gdf_outside\n",
    "\n",
    "\n",
    "def select_polygons(gdf_poly, gdf_object):\n",
    "    \"\"\"\n",
    "    Returns array of index values for polygons that contain an\n",
    "    other (e.g. bag) object. Returns a dataframe that contains an\n",
    "    object and a dataframe dat does not contain an object.\n",
    "    The second dataframe can be used for further selection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf_poly: geodataframe\n",
    "        Dataframe with the polygons to select\n",
    "\n",
    "    gdf_object: geodataframe\n",
    "        Dataframe with the (bag) objects used for the selection\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gdf_selected: geodataframe\n",
    "        Geodataframe with plots that do contain one or more object(s).\n",
    "    gdf_not_selected: geodataframe\n",
    "        Geodataframe with rest of the plots. This can be used\n",
    "        for further selection with other objects.\n",
    "    \"\"\"\n",
    "    select_index = []\n",
    "    for i, row in gdf_poly.iterrows():\n",
    "        if (gdf_object.within(row.geometry.convex_hull).any()):\n",
    "            #          print (\"plot met locaties erin: {}\".format(i))\n",
    "            select_index.append(i)\n",
    "\n",
    "    gdf_selected = gdf_poly.loc[gdf_poly.index.isin(select_index)]\n",
    "    gdf_not_selected = gdf_poly.loc[~gdf_poly.index.isin(select_index)]\n",
    "    return gdf_selected, gdf_not_selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataportaal > calc > utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Contains help functions for the check.\n",
    "Placed in a separate file because otherwise the main file becomes very\n",
    "long and messy.\n",
    "\"\"\"\n",
    "\n",
    "def create_test_bbox(gdf):\n",
    "    \"\"\"\n",
    "    Receives a geodataframe and determines the bounding box of it.\n",
    "    Next calculates a limited bounding box that is within the bounding\n",
    "    box of the given geodataframe. This limited bounding box is meant for\n",
    "    test purposes to limit the calculation time. \n",
    "    Returns the borders of this limited bounding box.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf: geodataframe\n",
    "        The geodataframe of which a test bounding box will be \n",
    "        calculated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xmin_test: int\n",
    "        The x of the lower left corner of the test bbox\n",
    "    ymin_test: int\n",
    "        The y of the lower left corner of the test bbox\n",
    "    xmax_test: int\n",
    "        The x of the upper right corner of the test bbox\n",
    "    ymax_test: int\n",
    "        The y of the upper right corner of the test bbox\n",
    "    gdf_test: geodataframe\n",
    "        The input geodataframe limited to the test bbox\n",
    "    polygon_test: shapely polygon\n",
    "        The test bbox as shapely polygon\n",
    "    \"\"\"\n",
    "    xmin = gdf.total_bounds[0]\n",
    "    xmax = gdf.total_bounds[2]\n",
    "\n",
    "    ymin = gdf.total_bounds[1]\n",
    "    ymax = gdf.total_bounds[3]\n",
    "\n",
    "    deltax = xmax - xmin\n",
    "    xmin_test = int(xmin + deltax / 2 - 500)\n",
    "    xmax_test = int(xmin_test + 1000)\n",
    "\n",
    "    deltay = ymax - ymin\n",
    "    ymin_test = int(ymin + deltay / 2 - 500)\n",
    "    ymax_test = int(ymin_test + 1000)\n",
    "\n",
    "    ## These test coordinates were used to solve a specific bug in some area.\n",
    "    # xmin_test = 93000\n",
    "    # ymin_test = 462500\n",
    "    # xmax_test = 94000\n",
    "    # ymax_test = 463500\n",
    "\n",
    "    polygon_test = f\"\"\"POLYGON(({xmin_test} {ymin_test},\n",
    "                                {xmin_test} {ymax_test},\n",
    "                                {xmax_test} {ymax_test},\n",
    "                                {xmax_test} {ymin_test},\n",
    "                                {xmin_test} {ymin_test}))\"\"\"\n",
    "\n",
    "    polygon_test= shapely.wkt.loads(polygon_test)\n",
    "    gdf_test = gpd.GeoDataFrame(index=[0],\n",
    "                                        crs='epsg:28992',\n",
    "                                        geometry=[polygon_test])\n",
    "    return xmin_test, ymin_test, xmax_test, ymax_test, \\\n",
    "           gdf_test, polygon_test\n",
    "\n",
    "\n",
    "def merge_shapes(municipalities, data_folder, filename):\n",
    "    \"\"\"Merges shapefiles that are created during the proces\n",
    "    (function ongerioleerde_percelen_check()) and writes the merged result to\n",
    "    a subdirectory 'common_shapes' in the given output folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    municipalities: array of str\n",
    "        An array with the names of the municipalitiess for which the calculation\n",
    "        should be performed.\n",
    "    data_folder: str\n",
    "        The directory in which all results are written, the check created for\n",
    "        each municipality a subdirectory. In this folder a new folder \n",
    "        'common_shapes' is created.\n",
    "    filename: str\n",
    "        The name of the shape that has to be merged. The check uses standard names\n",
    "        like 'buffer_total.shp' or 'plots_outside_buf.shp'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    result_path =  os.path.join(data_folder,\n",
    "                                    'common_shapes')\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "\n",
    "    merge_result = None\n",
    "    for municipality in municipalities:\n",
    "        filepath = os.path.join(data_folder, municipality, 'shapes', filename)\n",
    "        if os.path.exists(filepath):\n",
    "            gdf = gpd.read_file(filepath)\n",
    "            gdf['gemeente'] = municipality\n",
    "            if merge_result is None:\n",
    "                merge_result = gdf\n",
    "            else:\n",
    "                merge_result = merge_result.append(gdf)\n",
    "\n",
    "    # add_fields_and_save_as_shape(merge_result,\n",
    "    #                              os.path.join(result_path,\n",
    "    #                                           \"test_merged_\" + filename))\n",
    "    merge_result.to_file(os.path.join(result_path, \"merged_\" + filename))\n",
    "\n",
    "def add_fields_to_shape(path_to_shape):\n",
    "    # driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "    datasource = ogr.Open(path_to_shape, 1)  # 1 means read / write\n",
    "\n",
    "    fldDef = ogr.FieldDefn('Omschrijvi', ogr.OFTString)\n",
    "    fldDef.SetWidth(250)\n",
    "    fldDef.SetDefault(' ')\n",
    "    fldDef2 = ogr.FieldDefn('Maatregel', ogr.OFTString)\n",
    "    fldDef2.SetWidth(25)\n",
    "    fldDef.SetDefault(' ')\n",
    "    fldDef3 = ogr.FieldDefn('I.E.', ogr.OFTInteger)\n",
    "    fldDef3.SetWidth(8)\n",
    "    fldDef.SetDefault('0')\n",
    "\n",
    "    layer = datasource.GetLayer()\n",
    "    layer.CreateField(fldDef)\n",
    "    layer.CreateField(fldDef2)\n",
    "    layer.CreateField(fldDef3)\n",
    "\n",
    "\n",
    "def add_fields_and_save_as_shape(gdf, output_path):\n",
    "    \"\"\"\n",
    "    Add fields to the result shape file. Until now not successfull.\n",
    "\n",
    "    If fields are added the exported shape gives an error in ArcMap\n",
    "    This function is created to explore several options to avoid the\n",
    "    error. Not successfull until now.\n",
    "\n",
    "    It seems like using shape is not a good option.\n",
    "    \n",
    "    \"\"\"\n",
    "    # gdf[\"Omschrijvi\"] = '' \n",
    "    # gdf[\"Maatregel\"] = ''\n",
    "    # gdf[\"I.E.\"] = 0\n",
    "\n",
    "    # Get geopandas to populate a schema dict so we don't have to build it from scratch\n",
    "    schema = gpd.io.file.infer_schema(gdf)\n",
    "    # print(schema)\n",
    "    schema['properties']['Omschrijvi'] = 'str:250'  # 'Short integer' format\n",
    "    schema['properties']['Maatregel'] = 'str:25'  # 'Long integer' format\n",
    "    schema['properties']['I.E.'] = 'int32:10'  # 'Long integer' format\n",
    "    print(schema)\n",
    "    with open(output_path, 'w') as f:\n",
    "        # f.write(gdf.to_json())\n",
    "        f.write(gdf.to_file())\n",
    "    # gdf.to_file(output_path, schema=schema)\n",
    "    # print(fiona.drivers)\n",
    "    # add_fields_to_shape(output_path)\n",
    "\n",
    "def create_export(munlist, datapath):\n",
    "    \"\"\"\n",
    "    Help function to create export of all results of a given list \n",
    "    of municipalities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    munlist: array of str\n",
    "        The list of municipalities\n",
    "    datapath: str\n",
    "        The path to write all resultfiles to\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    outputf = os.path.join(datapath,\n",
    "                           'tmp_export')\n",
    "    if not os.path.exists(outputf):\n",
    "        os.makedirs(outputf)\n",
    "    for municipality in munlist:\n",
    "        input_path = os.path.join(datapath, municipality, \"shapes\")\n",
    "        gdf = gpd.read_file(os.path.join(input_path,\n",
    "                            \"plots_bag_outside_buf.shp\"))\n",
    "        gdf.to_file(os.path.join(outputf, f'{municipality}_result.shp'))\n",
    "\n",
    "def get_bag_objects(municipality, datapath):\n",
    "    \"\"\"\n",
    "    This function can be used in case the BAG service crashed (did happen few\n",
    "    times). If a municipality name is given, the exported shape with plots that\n",
    "    are outside the 40m buffer is loaded. After that the function\n",
    "    'check_all_plots_on_bag' is called for this dataframe.\n",
    "    Thus the opportunity is offered to get the final result for a \n",
    "    community in case of a crash in the end.\n",
    "\n",
    "    The final result is stored in the output directory for the given \n",
    "    municipality.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    municipality: str\n",
    "        The name of the municipality of which the calculation should be done.\n",
    "    datapath: str\n",
    "        The basepath where the results of all municipalities are stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    datapath = os.path.join(datapath,\n",
    "                            municipality,\n",
    "                            \"shapes\")\n",
    "    gdf = gpd.read_file(os.path.join(datapath,\n",
    "                        \"plots_outside_buf.shp\"))\n",
    "    logging.info('Checking for %s, %s plots on bag, this might take some time', \n",
    "                 municipality, len(gdf.lokaalID))\n",
    "\n",
    "    # Long names are abbreviated in a shape. Make sure to restore the\n",
    "    # names that are used in the function.\n",
    "    c_gemcode = 'kadastraleAanduiding|TypeKadastraleAanduiding|' + \\\n",
    "        'aKRKadastraleGemeenteCode|AKRKadastraleGemeenteCode|waarde'\n",
    "    c_perceelnr = 'perceelnummer'\n",
    "\n",
    "    gdf = gdf.rename(columns = {\"kadastra_3\": c_gemcode,\"perceelnum\": c_perceelnr})\n",
    "    result = check_allplots_on_bag(gdf)\n",
    "\n",
    "    result.to_file(os.path.join(\n",
    "                   datapath,\n",
    "                   \"plots_bag_outside_buf.shp\"))\n",
    "\n",
    "\n",
    "def filter_plots_with_bagobject(gdf):\n",
    "    \"\"\"\n",
    "    Receives an geodataframe with kadaster plot data. Uses function\n",
    "    find_address_plot for each row in the dataframe to check if that\n",
    "    plot has an address. If so, it will be selected otherwise it is removed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf: geodataframe\n",
    "        Contains kadaster plot data. Has at least these three fields:\n",
    "        'gemeente', 'sectie', 'perceelnummer'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    geodataframe with plots that have an BAG address.\n",
    "    ATTENTION:\n",
    "    In this version the addresses are not returned yet.\n",
    "    \"\"\"\n",
    "    gdf['BAG'] = False\n",
    "    for index, row in gdf.iterrows():\n",
    "        test = find_address_plot(row['gemeente'],\n",
    "                                 row['sectie'],\n",
    "                                 row['perceelnummer'])\n",
    "        if test is not None:\n",
    "            gdf.loc[gdf.index==index, 'BAG'] = True\n",
    "    return gdf.loc[gdf['BAG']]\n",
    "\n",
    "\n",
    "def find_address_plot(gemcode, sectie, number):\n",
    "    \"\"\"\n",
    "    Receives a plot id of a kadaster plot and uses the pdok webservice to find\n",
    "    the addresses for that plot if any. Returns a dataframe with the addresses\n",
    "    for the plot or None if no addresses are available\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gemcode: str\n",
    "        The community code (e.g. 'LDN01' for Leiden)\n",
    "    sectie: str\n",
    "        The letter for the kadastral section (e.g. 'R')\n",
    "    number: str\n",
    "        The number of the plot\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dataframe with the addresses that are found for the given parameters or\n",
    "    None if no addresses were found.\n",
    "    \"\"\"\n",
    "\n",
    "    url  = f\"https://geodata.nationaalgeoregister.nl/locatieserver/v3/suggest?\" +\\\n",
    "           f\"q=gekoppeld_perceel:{gemcode}-{sectie}-{number}&\" +\\\n",
    "           f\"fl=type,weergavenaam,id,score,gekoppeld_perceel\"\n",
    "\n",
    "    retValue = None\n",
    "    q = requests.get(url)\n",
    "    if (q.status_code == 200):\n",
    "        response = q.json()['response']\n",
    "        if response['numFound'] > 0:\n",
    "            retValue = pd.DataFrame(data=response['docs'])\n",
    "    return retValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataportaal > maps > get_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "if not cur_dir in sys.path:\n",
    "    sys.path.insert(0, cur_dir)\n",
    "\n",
    "# Start logging\n",
    "logging.basicConfig(\n",
    "    filename=f\"./{MUNICIPALITY.lower()}_get_data.log\",\n",
    "    level=logging.DEBUG,\n",
    "    filemode=\"w\",\n",
    ")\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "\n",
    "def run_opc_per_municipality(municipalities, data_folder, use_cache=True, test=True):\n",
    "    \"\"\"\n",
    "    Runs the check for a given list of municipalities,\n",
    "    reads and stores per municipality data in\n",
    "    <workingdirectory>\\\\data\\\\<municipality name>\n",
    "    Calls the function 'ongerioleerde_percelen_check' in which the\n",
    "    calculations are performed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    municipalities: array of str\n",
    "        The names of the municipalities to do the calculation for.\n",
    "    data_folder: string\n",
    "        Base path where the output should be written to.\n",
    "    usecache = bool, optional\n",
    "        If true checks if data already exist as a file in the data directory.\n",
    "        Default = True\n",
    "    test = bool, optional\n",
    "        If True, only performs calculation for a small area and not for the\n",
    "        given municipalities. Default = False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for municipality in municipalities:\n",
    "        logging.info(\n",
    "            \"==============> Processing municipality %s <===========\", municipality\n",
    "        )\n",
    "        ongerioleerde_percelen_check(\n",
    "            municipality,\n",
    "            os.path.join(data_folder, municipality),\n",
    "            use_cache=use_cache,\n",
    "            test=test,\n",
    "        )\n",
    "        logging.info(\n",
    "            \"==============> Finished %s successfully<===========\", municipality\n",
    "        )\n",
    "\n",
    "\n",
    "def read_data(municipality, data_folder, use_cache, test):\n",
    "    \"\"\"\n",
    "    Reads all data that are needed for the calculation from several web\n",
    "    services or -if available- from local files.\n",
    "    If use_cache is false, reads all data from the native source, otherwise\n",
    "    the data will be read from the given data_folder.\n",
    "    If downloaded, the data will be saved to '<data_folder>/<municipality>'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    municipality: str\n",
    "        The name of the municipality\n",
    "    data_folder: str\n",
    "        The base output directory\n",
    "    use_cache: bool\n",
    "        If true, it will be tried to read data from disk. If in that\n",
    "        case no data is found, it will be downloaded.\n",
    "    test: bool\n",
    "        If true a small area will be used to perform the calculation on.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gdf_plots: geodataframe\n",
    "        Contains all plots for the municipality. (from BRK)\n",
    "    gdf_sewer: geodataframe\n",
    "        Contains the sewer lines. (from GWSW)\n",
    "    gdf_pump: geodataframe\n",
    "        Contains sewer pumps (from GWSW)\n",
    "    gdf_water: geodataframe\n",
    "        Contains water polygons (from BGT)\n",
    "    gdf_railroads:\n",
    "        Contains railroad polygons (from BGT)\n",
    "    \"\"\"\n",
    "    # get the municipality contour\n",
    "    gdf_municipality, polygon_municipality = get_contour_municipalities(\n",
    "        municipality, data_folder, use_cache\n",
    "    )\n",
    "\n",
    "    if test:\n",
    "        # Choose a small area within the polygon of the municipality to limit\n",
    "        # the number of features for test purposes.\n",
    "        (\n",
    "            xmin_test,\n",
    "            ymin_test,\n",
    "            xmax_test,\n",
    "            ymax_test,\n",
    "            gdf_municipality,\n",
    "            polygon_municipality,\n",
    "        ) = ut.create_test_bbox(gdf_municipality)\n",
    "\n",
    "    if use_cache:\n",
    "        logging.info(\"Cache mode\")\n",
    "        try:\n",
    "            logging.info(\"Loading plots GeoDataFrame from cache\")\n",
    "            gdf_plots = gpd.read_parquet(os.path.join(data_folder, \"1_plots.parquet\"))\n",
    "\n",
    "        except IOError as error:\n",
    "            logging.info(\"Loading from cache failed, file not found. %s\", error)\n",
    "            gdf_plots = get_gdf_plots(\n",
    "                polygon_municipality, data_folder, cache_data=True\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            logging.info(\"Loading sewer lines GeoDataFrame from cache\")\n",
    "            gdf_sewer = gpd.read_parquet(\n",
    "                os.path.join(data_folder, \"2a_sewer_lines.parquet\")\n",
    "            )\n",
    "\n",
    "        except IOError as error:\n",
    "            logging.info(\"Loading from cache failed, file not found, %s\", error)\n",
    "            gdf_sewer = get_gwsw_sewer_lines(\n",
    "                str.replace(municipality, \" \", \"\"), data_folder, cache_data=True\n",
    "            )\n",
    "        try:\n",
    "            logging.info(\"Loading pump points GeoDataFrame from cache\")\n",
    "            gdf_pump = gpd.read_parquet(\n",
    "                os.path.join(data_folder, \"2b_pump_points.parquet\")\n",
    "            )\n",
    "\n",
    "        except IOError as error:\n",
    "            logging.info(\"Loading from cache failed, file not found, %s\", error)\n",
    "            gdf_pump = get_gwsw_pump_points(\n",
    "                str.replace(municipality, \" \", \"\"), data_folder, cache_data=True\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            logging.info(\"Loading BGT GeoDataFrame from cache\")\n",
    "            gdf_water = gpd.read_parquet(\n",
    "                os.path.join(data_folder, \"bgt_waterdeel.parquet\")\n",
    "            )\n",
    "            gdf_roads = gpd.read_parquet(\n",
    "                os.path.join(data_folder, \"bgt_wegdeel.parquet\")\n",
    "            )\n",
    "            gdf_railroads = gdf_roads.loc[gdf_roads[\"function\"] == \"spoorbaan\"]\n",
    "        except IOError as error:\n",
    "            logging.info(\"Loading BGT from cache failed, file not found\")\n",
    "            logging.info(\"Downloading BGT roads and water, %s.\", error)\n",
    "            bgt_dict = get_bgt_features_poly(\n",
    "                polygon_municipality.wkt, data_folder, cache_data=True\n",
    "            )\n",
    "            gdf_water = bgt_dict[\"bgt_waterdeel\"]\n",
    "            gdf_roads = bgt_dict[\"bgt_wegdeel\"]\n",
    "            # Attention: there are municipalities without railroads!\n",
    "            gdf_railroads = gdf_roads.loc[gdf_roads[\"function\"] == \"spoorbaan\"]\n",
    "    else:\n",
    "        logging.info(\"Downloading new data for municipality %s\", municipality)\n",
    "        logging.info(\"Downloading plots\")\n",
    "        gdf_plots = get_gdf_plots(\n",
    "            polygon_municipality, data_folder, cache_data=True\n",
    "        )\n",
    "        logging.info(\"Downloading sewer lines\")\n",
    "        gdf_sewer = get_gwsw_sewer_lines(\n",
    "            municipality, data_folder, cache_data=True\n",
    "        )\n",
    "        logging.info(\"Downloading pump points\")\n",
    "        gdf_pump = get_gwsw_pump_points(\n",
    "            municipality, data_folder, cache_data=True\n",
    "        )\n",
    "        logging.info(\"Downloading BGT roads and water\")\n",
    "        bgt_dict = get_bgt_features_poly(\n",
    "            polygon_municipality.wkt, data_folder, cache_data=True\n",
    "        )\n",
    "        gdf_water = bgt_dict[\"bgt_waterdeel\"]\n",
    "\n",
    "        logging.info(\"Extracting railroads from BGT roads.\")\n",
    "        gdf_roads = bgt_dict[\"bgt_wegdeel\"]\n",
    "        gdf_railroads = gdf_roads.loc[gdf_roads[\"function\"] == \"spoorbaan\"]\n",
    "\n",
    "    # ===== Some extra logging ============\n",
    "    if gdf_plots.empty:\n",
    "        logging.warning(\"Empty Plots GeoDataFrame found\")\n",
    "    if gdf_sewer.empty:\n",
    "        logging.warning(\"Empty Sewer GeoDataFrame found\")\n",
    "    if gdf_pump.empty:\n",
    "        logging.warning(\"Empty Pump GeoDataFrame found\")\n",
    "    # ===== Limit the data to the test bounding box if test is True =============\n",
    "    if test:\n",
    "        logging.info(\"Make test selection for development\")\n",
    "        gdf_sewer = gdf_sewer.to_crs(gdf_plots.crs)\n",
    "        gdf_pump = gdf_pump.to_crs(gdf_plots.crs)\n",
    "\n",
    "        # gdf_plots = gdf_plots.cx[xmin_test:xmax_test,\n",
    "        #                          ymin_test:ymax_test]\n",
    "        gdf_sewer = gdf_sewer.cx[xmin_test:xmax_test, ymin_test:ymax_test]\n",
    "        gdf_pump = gdf_pump.cx[xmin_test:xmax_test, ymin_test:ymax_test]\n",
    "        gdf_water = gdf_water.cx[xmin_test:xmax_test, ymin_test:ymax_test]\n",
    "        gdf_railroads = gdf_railroads.cx[xmin_test:xmax_test, ymin_test:ymax_test]\n",
    "    return gdf_plots, gdf_sewer, gdf_pump, gdf_water, gdf_railroads\n",
    "\n",
    "\n",
    "def ongerioleerde_percelen_check(municipality, data_folder, use_cache=False, test=True):\n",
    "    \"\"\"\n",
    "    Performs the check for a given municipality name. Downloaded data will be saved\n",
    "    to a directory: <data_folder>/<municipality>\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    municipality: str\n",
    "        The name of the municipality\n",
    "    data_folder: str\n",
    "        The base output directory\n",
    "    use_cache: bool\n",
    "        If true, it will be tried to read data from disk. If in that\n",
    "        case no data is found, it will be downloaded.\n",
    "    test: bool\n",
    "        If true a small area will be set to perform the test on.\n",
    "\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "\n",
    "    # Step 1: ====== Read the data ====================\n",
    "    gdf_plots, gdf_sewer, gdf_pump, gdf_water, gdf_railroads = read_data(\n",
    "        municipality, data_folder, use_cache, test\n",
    "    )\n",
    "\n",
    "    # This step is added because otherwise the selection with\n",
    "    # spatial indices goes wrong.\n",
    "    # gdf_plots = gdf_plots.reset_index()\n",
    "    # # create a folder to save the shape files\n",
    "    result_folder = os.path.join(data_folder, \"shapes\")\n",
    "    if not os.path.exists(result_folder):\n",
    "        os.makedirs(result_folder)\n",
    "\n",
    "    # Save the input files as shape.\n",
    "    gdf_plots.to_file(os.path.join(result_folder, \"kadaster_plots.shp\"))\n",
    "    gdf_sewer.to_file(os.path.join(result_folder, \"sewers.shp\"))\n",
    "    gdf_pump.to_file(os.path.join(result_folder, \"pumps.shp\"))\n",
    "    gdf_water.to_file(os.path.join(result_folder, \"bgt_water.shp\"))\n",
    "    if not gdf_railroads.empty:\n",
    "        gdf_railroads.to_file(os.path.join(result_folder, \"bgt_rail.shp\"))\n",
    "\n",
    "    # Step 2: =========== Create buffer for sewer and pumps ==========\n",
    "    logging.info(\n",
    "        \"Create polygon of the 40m buffer %s\", \" around sewer lines and pump points\"\n",
    "    )\n",
    "    gdf_pump_sewer = gpd.GeoDataFrame(pd.concat([gdf_pump, gdf_sewer])).reset_index()\n",
    "\n",
    "    buffer_total = gdf_pump_sewer.buffer(40)\n",
    "\n",
    "    # Step 3: ============ Create mask from the railroads and water ========\n",
    "    logging.info(\"Create mask from bgt to clip the buffer.\")\n",
    "    # Add water and railroads in one dataframe.\n",
    "    bgt_gdf = gpd.GeoDataFrame(\n",
    "        pd.concat(\n",
    "            [gdf_water[\"geometry\"], gdf_railroads[\"geometry\"]],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    ).reset_index()\n",
    "    bgt_gdf = gpd.GeoDataFrame(\n",
    "        pd.concat(\n",
    "            [gdf_water[\"geometry\"], gdf_railroads[\"geometry\"]],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    ).reset_index()\n",
    "\n",
    "    logging.info(\n",
    "        \"limit the number of bgt records using a spatial index to increase speed.\"\n",
    "    )\n",
    "    bgt_gdf_limited, bgt_excluded = select_items_with_spatial_index(\n",
    "        bgt_gdf, buffer_total\n",
    "    )\n",
    "\n",
    "    bgt_mask = bgt_gdf_limited.geometry.unary_union\n",
    "\n",
    "    # Step 4: ============== Clip the buffer with the created mask =============\n",
    "    logging.info(\"Clip the buffers (pump and sewer).\")\n",
    "    clipped_buffer = clip_buffer_select_part(buffer_total, bgt_mask, gdf_pump_sewer)\n",
    "\n",
    "    # export clipped buffer to shape.\n",
    "    gs_buffer_union = clipped_buffer.geometry.unary_union\n",
    "\n",
    "    # df_buffer_union = gpd.GeoDataFrame()\n",
    "    # df_buffer_union = df_buffer_union.set_geometry([gs_buffer_union])\n",
    "    # df_buffer_union = df_buffer_union.set_crs(\"EPSG:28992\")\n",
    "\n",
    "    try:\n",
    "        clipped_buffer.to_file(os.path.join(result_folder, \"clipped_buffer.shp\"))\n",
    "    except IOError as error:\n",
    "        logging.error(\"saving unary union of buffer failed. %s\", error)\n",
    "\n",
    "    # Step 5: =========== find all plots that are outside the clipped buffer =========\n",
    "    logging.info(\"Calculate plots outside the 40m buffer\")\n",
    "    logging.info(\"Limit plot candidates with spatial index\")\n",
    "    # Here the plots of with the spatial index is outside the buffer are\n",
    "    # excluded. The others are included.\n",
    "    # Using the clipped buffer here will result in selecting all plots!!\n",
    "    # Using the simple unclipped buffer gives the desired result.\n",
    "    gdf_plots_included, gdf_plots_excluded = select_items_with_spatial_index(\n",
    "        gdf_plots, buffer_total\n",
    "    )\n",
    "    # Here the plots of the included dataframe that are outside the buffer\n",
    "    # are selected.\n",
    "    logging.info(\"Calculating plots outside buffer...\")\n",
    "    gdf_outside_buf = gdf_plots_included[\n",
    "        gdf_plots_included.geometry.disjoint(gs_buffer_union)\n",
    "    ]\n",
    "\n",
    "    # Here all plots outside the buffer are joined in one dataframe\n",
    "    gdf_outside_buf = gpd.GeoDataFrame(\n",
    "        pd.concat([gdf_outside_buf, gdf_plots_excluded], ignore_index=True)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Export plots outside buffer to shape.\n",
    "    gdf_outside_buf.to_file(os.path.join(result_folder, \"plots_outside_buf.shp\"))\n",
    "\n",
    "    # TODO: Step 6: ============ Select plots that do not have an IBA =================\n",
    "    # Not implemented yet.\n",
    "    logging.info(\n",
    "        \"Select plots that do not have an IBA %s\",\n",
    "        \"(Individuele Behandeling Afvalwater)\",\n",
    "    )\n",
    "\n",
    "    # Step 7: ================== Only keep plots with a building which is in use ===============\n",
    "    logging.info(\"Add bag information to gdf\")\n",
    "    gdf_outside_buf = gdf_outside_buf.copy(deep=True)\n",
    "\n",
    "    # This is the check that uses the webservice kadaster - BAG webservice\n",
    "    # (use either this method, or the original check below):\n",
    "    # gdf_bag_outside_buf = api_kad.filter_plots_with_bagobject(gdf_outside_buf)\n",
    "    gdf_bag_outside_buf = check_allplots_on_bag(gdf_outside_buf)\n",
    "\n",
    "    if gdf_bag_outside_buf is None:\n",
    "        raise ValueError(\n",
    "            \"The building plots shape seems to be empty. Cannot proceed script.\"\n",
    "        )\n",
    "\n",
    "    # export result with bag information to shape\n",
    "    gdf_bag_outside_buf.to_file(\n",
    "        os.path.join(result_folder, \"plots_bag_outside_buf.shp\")\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Here the work is done. Calculations for municipalities Bodegraven Reeuwijk,\n",
    "    Haarlemmermeer, Alphen aan den Rijn take a lot of time (huge area). Normally\n",
    "    it is not possible to run all municipalities at once on a laptop because it\n",
    "    takes more than one day in total.\n",
    "    Also downloading plots for big municipalities sometimes results in errors\n",
    "    like 'too many requests'. Workarounds have been programmed for this, for\n",
    "    Haarlemmermeer it appeared to work.  But it is not an ideal situation of\n",
    "    course.\n",
    "\n",
    "    Variables listmunicipalities and existing municipalities were created to\n",
    "    split the calculations over serveral days. In ut.merge_shapes all results are\n",
    "    merged to one shape.\n",
    "    I always adapt the lists manually if more municipalities are calculated.\n",
    "    \"\"\"\n",
    "    # Initialize the logger. If no logging is required set level lower.\n",
    "    logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "\n",
    "    # The municipalities for which the check should be done.\n",
    "    listmunicipalities = [\n",
    "        \"Leiden\",\n",
    "        # \"Gouda\",\n",
    "        # \"Lisse\",\n",
    "        # \"Teylingen\",\n",
    "        # \"Hillegom\",\n",
    "        # \"Katwijk\",\n",
    "        # \"Wassenaar\",\n",
    "        # \"Leiderdorp\",\n",
    "        # \"Noordwijk\",\n",
    "        # \"Zoeterwoude\",\n",
    "        # \"Bodegraven-Reeuwijk\",\n",
    "        # \"Haarlemmermeer\",\n",
    "        # \"Alphen aan den Rijn\",\n",
    "    ]\n",
    "\n",
    "    # The municipalities that should be merged to the new output\n",
    "    # shape with all municipalities. (Kind of reminder.)\n",
    "    existingmunicipalities = []\n",
    "\n",
    "    output_folder = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "    ##########################################################\n",
    "    # Sometimes the bag service  does not work. This function\n",
    "    # does this bag analysis separately in case something went\n",
    "    # wrong.\n",
    "    # Saves a lot of calculation time because all previous\n",
    "    # analysis do not need to be done anew then.\n",
    "    # Exmple here is for Bodegraven-Reeuwijk.\n",
    "    # In bag service too an try except block is made to prevent\n",
    "    # a total crash after one error.\n",
    "    #########################################################\n",
    "    # ut.get_bag_objects('Bodegraven-Reeuwijk', output_folder)\n",
    "\n",
    "    #########################################################\n",
    "    # This is the real analysis, here all work is done.\n",
    "    #########################################################\n",
    "    run_opc_per_municipality(\n",
    "        listmunicipalities, output_folder, use_cache=True, test=False\n",
    "    )\n",
    "\n",
    "    ##########################################################\n",
    "    # In utilities is a function merge_shapes that is meant to\n",
    "    # merge final results to one shape.\n",
    "    # The function expects the municipality name as submap and\n",
    "    # looks for a given shape name in that map.\n",
    "    # (Final result for every municipality:\n",
    "    # plots_bag_outside_buf.shp)\n",
    "    ##########################################################\n",
    "    # ut.merge_shapes(existingmunicipalities,\n",
    "    #                 output_folder,\n",
    "    #                 'plots_bag_outside_buf.shp')\n",
    "\n",
    "    # ut.merge_shapes(existingmunicipalities,\n",
    "    #                 output_folder,\n",
    "    #                 'clipped_buffer.shp')\n",
    "\n",
    "    # Show some statistics like calculation time etc.\n",
    "    profiler.disable()\n",
    "    stats = pstats.Stats(profiler).sort_stats(\"cumulative\")\n",
    "    stats.print_stats(20)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot(BaseModel):\n",
    "    gml_id: str\n",
    "    polygon: List[Tuple[float, float]] = []\n",
    "\n",
    "    @property\n",
    "    def shapely_polygon(self):\n",
    "        return Polygon(self.polygon)\n",
    "\n",
    "\n",
    "class SewerLine(BaseModel):\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    gml_id: str\n",
    "    x1: float\n",
    "    y1: float\n",
    "    x2: float\n",
    "    y2: float\n",
    "    connected_pump_ids: List[str] = []\n",
    "\n",
    "    @property\n",
    "    def connected(self) -> bool:\n",
    "        return len(self.connected_pump_ids) > 0\n",
    "\n",
    "    @property\n",
    "    def shapely_linestring(self):\n",
    "        return LineString([(self.x1, self.y1), (self.x2, self.y2)])\n",
    "\n",
    "\n",
    "class Pump(BaseModel):\n",
    "    gml_id: str\n",
    "    x: float\n",
    "    y: float\n",
    "\n",
    "    connected_sewer_lines: List[SewerLine] = []\n",
    "    connected_plots: List[Plot] = []\n",
    "\n",
    "    @property\n",
    "    def num_connected_plots(self):\n",
    "        return len(self.connected_plots)\n",
    "\n",
    "    @property\n",
    "    def num_connected_sewer_lines(self):\n",
    "        return len(self.connected_sewer_lines)\n",
    "\n",
    "\n",
    "class WaterDeel(BaseModel):\n",
    "    gml_id: str\n",
    "    polygon: List[Tuple[float, float]] = []\n",
    "\n",
    "    @property\n",
    "    def shapely_polygon(self):\n",
    "        return Polygon(self.polygon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for municipality in MUNICIPALITIES:\n",
    "    print(f\"Downloading and preparing data for '{municipality}'\")\n",
    "    data_folder = f\"data/{municipality}\"\n",
    "    # create the data folder if it is not already done\n",
    "    Path(f\"./{data_folder}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Start logging\n",
    "    logging.basicConfig(\n",
    "        filename=f\"./{data_folder}/{municipality.lower()}_get_data.log\",\n",
    "        level=logging.INFO,\n",
    "        filemode=\"w\",\n",
    "    )\n",
    "    logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "\n",
    "    profiler.disable()\n",
    "    stats = pstats.Stats(profiler).sort_stats(\"cumulative\")\n",
    "    stats.print_stats(20)\n",
    "\n",
    "    gdf_municipality, polygon_municipality = get_contour_municipalities(\n",
    "        municipality, data_folder, False\n",
    "    )\n",
    "\n",
    "    logging.info(\"Downloading new data for municipality %s\", municipality)\n",
    "    logging.info(\"Downloading plots\")\n",
    "    gdf_plots = get_gdf_plots(\n",
    "        polygon_municipality, data_folder, cache_data=True\n",
    "    )\n",
    "    logging.info(\"Downloading sewer lines\")\n",
    "    gdf_sewer = get_gwsw_sewer_lines(\n",
    "        municipality, data_folder, cache_data=True\n",
    "    )\n",
    "    logging.info(\"Downloading pump points\")\n",
    "    gdf_pump = get_gwsw_pump_points(municipality, data_folder, cache_data=True)\n",
    "    logging.info(\"Downloading BGT roads and water\")\n",
    "    bgt_dict = get_bgt_features_poly(\n",
    "        polygon_municipality.wkt, data_folder, cache_data=True\n",
    "    )\n",
    "    gdf_water = bgt_dict[\"bgt_waterdeel\"]\n",
    "\n",
    "    logging.info(\"Extracting railroads from BGT roads.\")\n",
    "    gdf_roads = bgt_dict[\"bgt_wegdeel\"]\n",
    "    gdf_railroads = gdf_roads.loc[gdf_roads[\"function\"] == \"spoorbaan\"]\n",
    "\n",
    "    # ===== Some extra logging ============\n",
    "    if gdf_plots.empty:\n",
    "        logging.warning(\"Empty Plots GeoDataFrame found\")\n",
    "    if gdf_sewer.empty:\n",
    "        logging.warning(\"Empty Sewer GeoDataFrame found\")\n",
    "    if gdf_pump.empty:\n",
    "        logging.warning(\"Empty Pump GeoDataFrame found\")\n",
    "\n",
    "    gdf_plots.to_file(str(Path(data_folder) / \"01_plots.shp\"))\n",
    "    gdf_sewer.to_file(str(Path(data_folder) / \"02_sewerlines.shp\"))\n",
    "    gdf_pump.to_file(str(Path(data_folder) / \"03_pumps.shp\"))\n",
    "    gdf_water.to_file(str(Path(data_folder) / \"04_water.shp\"))\n",
    "    gdf_roads.to_file(str(Path(data_folder) / \"05_roads.shp\"))\n",
    "    gdf_railroads.to_file(str(Path(data_folder) / \"06_railroads.shp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rec_connect_closest_sewerline(\n",
    "    pump: Pump,\n",
    "    sewerlines: List[SewerLine],\n",
    "    x: float,\n",
    "    y: float,\n",
    "    max_distance: float,\n",
    "):\n",
    "    for i, sl in enumerate(sewerlines):\n",
    "        if pump.gml_id in sl.connected_pump_ids:\n",
    "            continue\n",
    "        dl1 = hypot(sl.x1 - x, sl.y1 - y)\n",
    "        dl2 = hypot(sl.x2 - x, sl.y2 - y)\n",
    "        if min([dl1, dl2]) < max_distance:\n",
    "            sewerlines[i].connected_pump_ids.append(pump.gml_id)\n",
    "            pump.connected_sewer_lines.append(sewerlines[i])\n",
    "            _rec_connect_closest_sewerline(\n",
    "                pump, sewerlines, sl.x1, sl.y1, MAX_SEWER_LINE_TO_SEWER_LINE_DISTANCE\n",
    "            )\n",
    "            _rec_connect_closest_sewerline(\n",
    "                pump, sewerlines, sl.x2, sl.y2, MAX_SEWER_LINE_TO_SEWER_LINE_DISTANCE\n",
    "            )\n",
    "\n",
    "\n",
    "for municipality in MUNICIPALITIES:\n",
    "    data_folder = f\"data/{municipality}\"\n",
    "    analyse_folder = f\"analyse/{municipality}\"\n",
    "    pumps = []\n",
    "    plots = []\n",
    "    waterdelen = []\n",
    "    sewerlines = []\n",
    "\n",
    "    Path(analyse_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        filename=f\"{analyse_folder}/{municipality.lower()}_analyse.log\",\n",
    "        level=logging.DEBUG,\n",
    "        filemode=\"w\",\n",
    "    )\n",
    "    logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "    ##########################\n",
    "    # STAP 1A - pompen laden #\n",
    "    ##########################\n",
    "    try:\n",
    "        gdf_pumps = gpd.read_parquet(Path(data_folder) / \"2b_pump_points.parquet\")\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"Cannot find the file '2b_pump_points.parquet' in the given path '{data_folder}'\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    for _, r in gdf_pumps.iterrows():\n",
    "        if type(r.geometry) == MultiPoint:\n",
    "            geoms = r.geometry.geoms\n",
    "        else:\n",
    "            geoms = [r.geometry]\n",
    "\n",
    "        for geom in geoms:\n",
    "            x, y = [\n",
    "                n[0] for n in geom.xy\n",
    "            ]  # coords won't work because of Point Z types mixed with Point types, xy will return 2 numpy arrays\n",
    "            gml_id = r.id\n",
    "            if gml_id is None:\n",
    "                logging.info(f\"Found pump without gml_id at x={x:.2f}, y={y:.2f}\")\n",
    "            else:\n",
    "                pumps.append(Pump(gml_id=gml_id, x=x, y=y))\n",
    "\n",
    "    logging.info(f\"Found {len(pumps)} pumps.\")\n",
    "\n",
    "    #########################\n",
    "    # STAP 1B - plots laden #\n",
    "    #########################\n",
    "    try:\n",
    "        gdf_plots = gpd.read_parquet(Path(data_folder) / \"1_plots.parquet\")\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"Cannot find the file '1_plots.parquet' in the given path '{data_folder}'\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    for _, r in gdf_plots.iterrows():\n",
    "        gml_id = r.gml_id\n",
    "        xs, ys = r.geometry.exterior.coords.xy\n",
    "        if gml_id is None:\n",
    "            logging.info(f\"Found plot without gml_id at {r.geometry.centroid}\")\n",
    "        else:\n",
    "            plots.append(\n",
    "                Plot(\n",
    "                    gml_id=gml_id,\n",
    "                    polygon=[p for p in zip(xs, ys)],\n",
    "                )\n",
    "            )\n",
    "\n",
    "    logging.info(f\"Found {len(plots)} plots.\")\n",
    "\n",
    "    ##############################\n",
    "    # STAP 1C - waterdelen laden #\n",
    "    ##############################\n",
    "    try:\n",
    "        gdf_waterdelen = gpd.read_parquet(Path(data_folder) / \"bgt_waterdeel.parquet\")\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"Cannot find the file 'bgt_waterdeels.parquet' in the given path '{data_folder}'\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    for _, r in gdf_waterdelen.iterrows():\n",
    "        gml_id = r.gml_id\n",
    "        xs, ys = r.geometry.exterior.coords.xy\n",
    "\n",
    "        waterdelen.append(\n",
    "            WaterDeel(\n",
    "                gml_id=gml_id,\n",
    "                polygon=[p for p in zip(xs, ys)],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # gdf_waterdelen = gpd.GeoDataFrame(\n",
    "    #     geometry=[wd.shapely_polygon for wd in waterdelen], crs=\"EPSG:28992\"\n",
    "    # )\n",
    "\n",
    "    logging.info(f\"Found {len(waterdelen)} waterparts.\")\n",
    "\n",
    "    #############################\n",
    "    # STAP 1D - leidingen laden #\n",
    "    #############################\n",
    "    try:\n",
    "        gdf_sewerlines = gpd.read_parquet(Path(data_folder) / \"2a_sewer_lines.parquet\")\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"Cannot find the file '2a_sewer_lines.parquet' in the given path '{data_folder}'\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    for _, row in gdf_sewerlines.iterrows():\n",
    "        for geom in row.geometry.geoms:\n",
    "            for i in range(0, len(geom.coords) - 1, 2):\n",
    "                x1, y1 = geom.coords[i][0], geom.coords[i][1]\n",
    "                x2, y2 = geom.coords[i + 1][0], geom.coords[i + 1][1]\n",
    "                if row.id is None:\n",
    "                    logging.info(f\"Found pump without gml_id at x={x1:.2f}, y={y1:.2f}\")\n",
    "                else:\n",
    "                    sewerlines.append(\n",
    "                        SewerLine(\n",
    "                            gml_id=row.id,\n",
    "                            x1=round(x1, 2),\n",
    "                            y1=round(y1, 2),\n",
    "                            x2=round(x2, 2),\n",
    "                            y2=round(y2, 2),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    logging.info(f\"Found {len(sewerlines)} sewerlines.\")\n",
    "\n",
    "    #######################################################\n",
    "    # STAP 2 - Verwijder leidingen die waterdelen snijden #\n",
    "    #######################################################\n",
    "\n",
    "    #############################################\n",
    "    # STAP 2A - maak 1 object van de waterdelen #\n",
    "    #############################################\n",
    "    waterdelen_filename = f\"{analyse_folder}/02A_waterdelen_union.parquet\"\n",
    "    if not Path(waterdelen_filename).exists() or FORCE_RELOAD:\n",
    "        logging.info(f\"Creating combined waterdelen shape.\")\n",
    "        union_waterdelen = gdf_waterdelen.union_all()\n",
    "        gdf_waterdelen.to_parquet(waterdelen_filename)\n",
    "    else:\n",
    "        logging.info(f\"Reading combined waterdelen shape...\")\n",
    "        gdf_waterdelen = gpd.read_parquet(waterdelen_filename)\n",
    "        union_waterdelen = gdf_waterdelen.union_all()\n",
    "\n",
    "    ############################################################\n",
    "    # STAP 2B - verwijder leidingen die snijden met waterdelen #\n",
    "    ############################################################\n",
    "    filtered_sewerlines_filename = f\"{analyse_folder}/02B_filtered_sewerlines.shp\"\n",
    "\n",
    "    if not Path(filtered_sewerlines_filename).exists() or FORCE_RELOAD:\n",
    "        logging.info(\"Filtering sewerlines that intersect with the waterdelen objects.\")\n",
    "        filtered_sewerlines = []\n",
    "        print(\"Filtering sewerlines that intersect with the waterdelen objects....\")\n",
    "        for sewerline in tqdm(sewerlines):\n",
    "            if sewerline.shapely_linestring.intersects(union_waterdelen):\n",
    "                logging.info(\n",
    "                    f\"Sewerline '{sewerline.gml_id}' intersects with a waterpart so it is removed from the analysis.\"\n",
    "                )\n",
    "            else:\n",
    "                filtered_sewerlines.append(sewerline)\n",
    "\n",
    "        # create a shape for later access so we can skip this step\n",
    "        w = shapefile.Writer(filtered_sewerlines_filename)\n",
    "        w.field(\"id\")\n",
    "        for sewerline in filtered_sewerlines:\n",
    "            w.line(\n",
    "                [\n",
    "                    [\n",
    "                        (sewerline.x1, sewerline.y1),\n",
    "                        (sewerline.x2, sewerline.y2),\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "            w.record(sewerline.gml_id)\n",
    "        w.close()\n",
    "        sewerlines = [o for o in filtered_sewerlines]\n",
    "    else:\n",
    "        logging.info(\n",
    "            f\"Reading sewerlines that do not intersect with the waterdelen objects.\"\n",
    "        )\n",
    "        sewerlines = []\n",
    "        try:\n",
    "            gdf_sewerlines_filtered = gpd.read_file(filtered_sewerlines_filename)\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Cannot find the file '{filtered_sewerlines_filename}' in the given path '{data_folder}'\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # reload the sewerlines, this time\n",
    "        for _, row in gdf_sewerlines_filtered.iterrows():\n",
    "            geometry = row[\"geometry\"]\n",
    "            coords = list(geometry.coords)\n",
    "            sewerlines.append(\n",
    "                SewerLine(\n",
    "                    gml_id=row[\"id\"],\n",
    "                    x1=coords[0][0],\n",
    "                    y1=coords[0][1],\n",
    "                    x2=coords[1][0],\n",
    "                    y2=coords[1][1],\n",
    "                )\n",
    "            )\n",
    "\n",
    "    logging.info(f\"Found {len(sewerlines)} sewerlines not crossing any waterparts.\")\n",
    "\n",
    "    ########################################\n",
    "    # STAP 3 - Koppelen leiding met pompen #\n",
    "    ########################################\n",
    "    connected_sewerlines_filename = f\"{analyse_folder}/03_connected_sewerlines.shp\"\n",
    "    if not Path(connected_sewerlines_filename).exists() or FORCE_RELOAD:\n",
    "        w = shapefile.Writer(connected_sewerlines_filename)\n",
    "        w.field(\"id\")\n",
    "        w.field(\"pump_id\")\n",
    "\n",
    "        print(\"Connecting pumps and sewerlines...\")\n",
    "        for pump in tqdm(pumps):\n",
    "            _rec_connect_closest_sewerline(\n",
    "                pump, sewerlines, pump.x, pump.y, MAX_PUMP_TO_SEWERLINE_DISTANCE\n",
    "            )\n",
    "            if len(pump.connected_sewer_lines) > 0:\n",
    "                Path(f\"{analyse_folder}/debug\").mkdir(parents=True, exist_ok=True)\n",
    "                sewerline_filename = f\"{analyse_folder}/debug/03_pump_{pump.gml_id}.shp\"\n",
    "                w_pump = shapefile.Writer(sewerline_filename)\n",
    "                w_pump.field(\"sewerline_id\")\n",
    "                for sl in pump.connected_sewer_lines:\n",
    "                    w_pump.line([[(sl.x1, sl.y1), (sl.x2, sl.y2)]])\n",
    "                    w_pump.record(sl.gml_id)\n",
    "                w_pump.close()\n",
    "\n",
    "            for sl in pump.connected_sewer_lines:\n",
    "                w.line([[(sl.x1, sl.y1), (sl.x2, sl.y2)]])\n",
    "                w.record(sl.gml_id, pump.gml_id)\n",
    "        w.close()\n",
    "    else:\n",
    "        print(\"Reading connected sewerlines...\")\n",
    "        connected_sewerlines = []\n",
    "        try:\n",
    "            gdf_connected_sewerlines = gpd.read_file(connected_sewerlines_filename)\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Cannot find the file '{connected_sewerlines_filename}' in the given path '{data_folder}'\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # reload the sewerlines, this time with the connections\n",
    "        for _, row in tqdm(gdf_connected_sewerlines.iterrows()):\n",
    "            for sl in sewerlines:\n",
    "                if sl.gml_id == row[\"id\"]:\n",
    "                    sl.connected_pump_ids.append(row[\"pump_id\"])\n",
    "                    for pump in pumps:\n",
    "                        if pump.gml_id == row[\"pump_id\"]:\n",
    "                            pump.connected_sewer_lines.append(sl.gml_id)\n",
    "                            break\n",
    "                    break\n",
    "\n",
    "    ##\n",
    "    connected_sewerlines = [\n",
    "        sewerline for sewerline in sewerlines if sewerline.connected\n",
    "    ]\n",
    "    logging.info(f\"Found {len(connected_sewerlines)} connected sewerlines.\")\n",
    "\n",
    "    #################################################\n",
    "    # STAP 4 - Per pomp aangesloten percelen vinden #\n",
    "    #################################################\n",
    "    logging.info(f\"Creating a buffer around the pumps and connected sewerlines.\")\n",
    "\n",
    "    sewerline_data = {\"id\": [], \"geometry\": []}\n",
    "    for sewerline in connected_sewerlines:\n",
    "        ls = LineString([[sewerline.x1, sewerline.y1], [sewerline.x2, sewerline.y2]])\n",
    "        buffer = ls.buffer(MAX_SEWER_LINE_TO_PLOT_DISTANCE)\n",
    "        sewerline_data[\"id\"].append(sewerline.gml_id)\n",
    "        sewerline_data[\"geometry\"].append(buffer)\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(sewerline_data, crs=\"EPSG:28992\")\n",
    "    gdf.to_file(Path(analyse_folder) / \"04A_sewerlines_with_buffer.shp\")\n",
    "\n",
    "    # now combine the geometries of the pump sewerlines with the buffer to one polygon with\n",
    "    # the pump id as the field\n",
    "    dissolved_gdf = gdf.dissolve()\n",
    "    dissolved_gdf.to_file(\n",
    "        Path(analyse_folder) / \"04B_sewerlines_with_buffer_dissolved.shp\"\n",
    "    )\n",
    "\n",
    "    # subtract the water of the buffered lines\n",
    "    dissolved_water_gdf = gdf_waterdelen.dissolve()\n",
    "    dissolved_water_gdf.to_file(Path(analyse_folder) / \"04C_waterdelen_combined.shp\")\n",
    "\n",
    "    subtracted_water_gdf = gpd.overlay(\n",
    "        dissolved_gdf, gdf_waterdelen, how=\"difference\", keep_geom_type=True\n",
    "    )\n",
    "    subtracted_water_gdf.to_file(\n",
    "        Path(analyse_folder) / \"04D_sewerlines_with_buffer_water_subtracted.shp\"\n",
    "    )\n",
    "\n",
    "    # create individual polygons out of this dataframe\n",
    "    individual_polygons = []\n",
    "    for _, row in subtracted_water_gdf.iterrows():\n",
    "        geom = row[\"geometry\"]\n",
    "        if isinstance(geom, MultiPolygon):\n",
    "            for poly in geom.geoms:\n",
    "                individual_polygons.append({\"geometry\": poly})\n",
    "        else:\n",
    "            individual_polygons.append({\"geometry\": geom})\n",
    "\n",
    "    gdf_sewerlines = gpd.GeoDataFrame(individual_polygons, crs=\"EPSG:28992\")\n",
    "    gdf_sewerlines.to_file(\n",
    "        Path(analyse_folder) / \"04E_individual_sewerline_polygons.shp\"\n",
    "    )\n",
    "\n",
    "    # remove all the polygons that do not contain an active pump\n",
    "    active_pumps = [p for p in pumps if p.num_connected_sewer_lines > 0]\n",
    "\n",
    "    # create a geodataframe for the active pumps\n",
    "    gdf_active_pumps = gpd.GeoDataFrame(\n",
    "        {\"geometry\": [Point(p.x, p.y) for p in active_pumps]},\n",
    "        index=[p.gml_id for p in active_pumps],\n",
    "        crs=\"EPSG:28992\",\n",
    "    )\n",
    "    gdf_active_pumps.to_file(Path(analyse_folder) / \"04F_active_pumps.shp\")\n",
    "\n",
    "    gdf_join = gpd.sjoin(\n",
    "        gdf_sewerlines, gdf_active_pumps, how=\"inner\", predicate=\"contains\"\n",
    "    )\n",
    "    gdf_join.to_file(Path(analyse_folder) / \"04G_active_areas.shp\")\n",
    "\n",
    "    # now find all plots that intersect with these active areas\n",
    "    gdf_join = gdf_join.drop([\"index_right\"], axis=1)\n",
    "    gdf_result = gpd.sjoin(gdf_plots, gdf_join, how=\"inner\")\n",
    "\n",
    "    gdf_result.to_file(Path(analyse_folder) / \"05_plots_with_sewer.shp\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
